"""
Challenge 3: å¹¶è¡Œå¤„ç†å’Œå­å›¾

å­¦ä¹ ç›®æ ‡:
- æŒæ¡å¹¶è¡ŒèŠ‚ç‚¹æ‰§è¡Œ
- å­¦ä¹ å­å›¾(Subgraph)è®¾è®¡
- ç†è§£å¤æ‚å·¥ä½œæµç¼–æ’
- å®ç°ç»“æœèšåˆç­–ç•¥

æ ¸å¿ƒæ¦‚å¿µ:
1. å¹¶è¡ŒèŠ‚ç‚¹å¤„ç† - åŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡
2. å­å›¾åµŒå¥— - æ¨¡å—åŒ–å·¥ä½œæµè®¾è®¡
3. çŠ¶æ€åˆå¹¶ - å¤šä¸ªç»“æœçš„æ•´åˆ
4. æ€§èƒ½ä¼˜åŒ– - å¹¶è¡Œæ‰§è¡Œçš„ä¼˜åŠ¿
"""

import os
import asyncio
import json
import time
from datetime import datetime
from typing import TypedDict, Annotated, List
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if not os.getenv("OPENAI_API_KEY"):
    print("âš ï¸  è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    exit(1)

# 1. å®šä¹‰çŠ¶æ€ç»“æ„
class AnalysisState(TypedDict):
    """æ•°æ®åˆ†æçŠ¶æ€"""
    messages: Annotated[list, add_messages]
    raw_data: str  # åŸå§‹æ•°æ®
    data_summary: str  # æ•°æ®æ‘˜è¦
    chart_description: str  # å›¾è¡¨æè¿°
    insights: str  # æ•°æ®æ´å¯Ÿ
    report: str  # æœ€ç»ˆæŠ¥å‘Š
    processing_time: dict  # å¤„ç†æ—¶é—´è®°å½•

class TaskState(TypedDict):
    """å­ä»»åŠ¡çŠ¶æ€"""
    task_id: str
    input_data: str
    result: str
    processing_time: float
    status: str

# 2. åˆå§‹åŒ–LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

# 3. æ¨¡æ‹Ÿæ•°æ®æº
SAMPLE_DATA = {
    "sales": """
    äº§å“é”€å”®æ•°æ® (Q1 2024):
    - iPhone: 1200ä¸‡å°, æ”¶å…¥: 180äº¿ç¾å…ƒ
    - MacBook: 400ä¸‡å°, æ”¶å…¥: 80äº¿ç¾å…ƒ  
    - iPad: 800ä¸‡å°, æ”¶å…¥: 60äº¿ç¾å…ƒ
    - Watch: 600ä¸‡å°, æ”¶å…¥: 20äº¿ç¾å…ƒ
    å¢é•¿ç‡: iPhone(+15%), MacBook(+8%), iPad(-3%), Watch(+25%)
    """,
    
    "user_behavior": """
    ç”¨æˆ·è¡Œä¸ºæ•°æ®:
    - æ—¥æ´»è·ƒç”¨æˆ·: 280ä¸‡
    - å¹³å‡ä½¿ç”¨æ—¶é•¿: 45åˆ†é’Ÿ
    - ç”¨æˆ·ç•™å­˜ç‡: 85%
    - æœ€å—æ¬¢è¿åŠŸèƒ½: æœç´¢(60%), æ¨è(25%), ç¤¾äº¤(15%)
    åœ°åŸŸåˆ†å¸ƒ: åŒ—äº¬(30%), ä¸Šæµ·(25%), æ·±åœ³(20%), å…¶ä»–(25%)
    """,
    
    "market": """
    å¸‚åœºæ•°æ®:
    - è¡Œä¸šæ€»è§„æ¨¡: 1000äº¿ç¾å…ƒ
    - å¸‚åœºä»½é¢: å…¬å¸A(25%), å…¬å¸B(20%), å…¬å¸C(18%), å…¶ä»–(37%)
    - å¢é•¿è¶‹åŠ¿: +12% YoY
    - ä¸»è¦é©±åŠ¨å› ç´ : AIæŠ€æœ¯é‡‡ç”¨, ç§»åŠ¨ç«¯æ™®åŠ, äº‘æœåŠ¡éœ€æ±‚
    """
}

# 4. å¹¶è¡Œåˆ†æèŠ‚ç‚¹
def data_summary_node(state: AnalysisState) -> dict:
    """æ•°æ®æ‘˜è¦èŠ‚ç‚¹ - å¹¶è¡Œä»»åŠ¡1"""
    print("ğŸ“Š [æ•°æ®æ‘˜è¦] åˆ†ææ•°æ®æ¦‚å†µ...")
    start_time = time.time()
    
    prompt = f"""è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œæ‘˜è¦åˆ†æ:

{state['raw_data']}

è¦æ±‚:
1. è¯†åˆ«å…³é”®æ•°æ®ç‚¹
2. è®¡ç®—ä¸»è¦æŒ‡æ ‡
3. ç”Ÿæˆç®€æ´æ‘˜è¦

è¿”å›æ ¼å¼: æ•°æ®æ‘˜è¦æ–‡æœ¬"""
    
    response = llm.invoke([{"role": "user", "content": prompt}])
    processing_time = time.time() - start_time
    
    print(f"   å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
    
    return {
        "data_summary": response.content,
        "processing_time": {
            **state.get("processing_time", {}),
            "data_summary": processing_time
        }
    }

def chart_description_node(state: AnalysisState) -> dict:
    """å›¾è¡¨æè¿°èŠ‚ç‚¹ - å¹¶è¡Œä»»åŠ¡2"""
    print("ğŸ“ˆ [å›¾è¡¨æè¿°] ç”Ÿæˆå¯è§†åŒ–å»ºè®®...")
    start_time = time.time()
    
    prompt = f"""åŸºäºä»¥ä¸‹æ•°æ®ï¼Œè®¾è®¡åˆé€‚çš„å›¾è¡¨å’Œå¯è§†åŒ–æ–¹æ¡ˆ:

{state['raw_data']}

è¦æ±‚:
1. æ¨èæœ€ä½³å›¾è¡¨ç±»å‹
2. ç¡®å®šå…³é”®å¯è§†åŒ–ç»´åº¦
3. å»ºè®®äº¤äº’åŠŸèƒ½
4. è®¾è®¡é¢œè‰²å’Œå¸ƒå±€

è¿”å›æ ¼å¼: å›¾è¡¨è®¾è®¡æè¿°"""
    
    response = llm.invoke([{"role": "user", "content": prompt}])
    processing_time = time.time() - start_time
    
    print(f"   å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
    
    return {
        "chart_description": response.content,
        "processing_time": {
            **state.get("processing_time", {}),
            "chart_description": processing_time
        }
    }

def insights_generation_node(state: AnalysisState) -> dict:
    """æ´å¯Ÿç”ŸæˆèŠ‚ç‚¹ - å¹¶è¡Œä»»åŠ¡3"""
    print("ğŸ’¡ [æ´å¯Ÿç”Ÿæˆ] æŒ–æ˜æ•°æ®æ´å¯Ÿ...")
    start_time = time.time()
    
    prompt = f"""æ·±åº¦åˆ†æä»¥ä¸‹æ•°æ®ï¼ŒæŒ–æ˜æœ‰ä»·å€¼çš„ä¸šåŠ¡æ´å¯Ÿ:

{state['raw_data']}

è¦æ±‚:
1. è¯†åˆ«è¶‹åŠ¿å’Œæ¨¡å¼
2. å‘ç°å¼‚å¸¸å’Œæœºä¼š
3. æä¾›å¯è¡Œæ€§å»ºè®®
4. é¢„æµ‹æœªæ¥å‘å±•

è¿”å›æ ¼å¼: æ·±åº¦æ´å¯Ÿåˆ†æ"""
    
    response = llm.invoke([{"role": "user", "content": prompt}])
    processing_time = time.time() - start_time
    
    print(f"   å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
    
    return {
        "insights": response.content,
        "processing_time": {
            **state.get("processing_time", {}),
            "insights": processing_time
        }
    }

# 5. å­å›¾ - æ•°æ®é¢„å¤„ç†å·¥ä½œæµ
def create_preprocessing_subgraph():
    """åˆ›å»ºæ•°æ®é¢„å¤„ç†å­å›¾"""
    
    def data_validation_node(state: TaskState) -> dict:
        """æ•°æ®éªŒè¯èŠ‚ç‚¹"""
        print(f"   ğŸ” [éªŒè¯] ä»»åŠ¡ {state['task_id']}")
        time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        return {
            "result": f"æ•°æ®éªŒè¯å®Œæˆ: {state['input_data'][:30]}...",
            "status": "validated"
        }
    
    def data_cleaning_node(state: TaskState) -> dict:
        """æ•°æ®æ¸…æ´—èŠ‚ç‚¹"""
        print(f"   ğŸ§¹ [æ¸…æ´—] ä»»åŠ¡ {state['task_id']}")
        time.sleep(0.3)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        return {
            "result": f"æ•°æ®æ¸…æ´—å®Œæˆ: ç§»é™¤å¼‚å¸¸å€¼å’Œé‡å¤é¡¹",
            "status": "cleaned"
        }
    
    def data_transformation_node(state: TaskState) -> dict:
        """æ•°æ®è½¬æ¢èŠ‚ç‚¹"""
        print(f"   ğŸ”„ [è½¬æ¢] ä»»åŠ¡ {state['task_id']}")
        time.sleep(0.4)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        return {
            "result": f"æ•°æ®è½¬æ¢å®Œæˆ: æ ‡å‡†åŒ–æ ¼å¼",
            "status": "transformed"
        }
    
    # æ„å»ºå­å›¾
    subgraph = StateGraph(TaskState)
    subgraph.add_node("validate", data_validation_node)
    subgraph.add_node("clean", data_cleaning_node)
    subgraph.add_node("transform", data_transformation_node)
    
    subgraph.add_edge(START, "validate")
    subgraph.add_edge("validate", "clean")
    subgraph.add_edge("clean", "transform")
    subgraph.add_edge("transform", END)
    
    return subgraph.compile()

# 6. ä¸»å·¥ä½œæµèŠ‚ç‚¹
def data_preprocessing_node(state: AnalysisState) -> dict:
    """æ•°æ®é¢„å¤„ç†èŠ‚ç‚¹ - ä½¿ç”¨å­å›¾"""
    print("ğŸ”§ [æ•°æ®é¢„å¤„ç†] æ‰§è¡Œé¢„å¤„ç†å­å›¾...")
    start_time = time.time()
    
    # åˆ›å»ºé¢„å¤„ç†å­å›¾
    preprocessor = create_preprocessing_subgraph()
    
    # ä¸ºæ¯ç§æ•°æ®ç±»å‹åˆ›å»ºå­ä»»åŠ¡
    data_types = ["sales", "user_behavior", "market"]
    results = []
    
    for i, data_type in enumerate(data_types):
        task_state: TaskState = {
            "task_id": f"preprocess_{data_type}",
            "input_data": SAMPLE_DATA.get(data_type, ""),
            "result": "",
            "processing_time": 0.0,
            "status": "pending"
        }
        
        print(f"   å¤„ç† {data_type} æ•°æ®...")
        result = preprocessor.invoke(task_state)
        results.append(result["result"])
    
    processing_time = time.time() - start_time
    preprocessed_data = "\n".join(results)
    
    print(f"   é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
    
    return {
        "raw_data": state["raw_data"] + f"\n\né¢„å¤„ç†ç»“æœ:\n{preprocessed_data}",
        "processing_time": {
            **state.get("processing_time", {}),
            "preprocessing": processing_time
        }
    }

def report_generation_node(state: AnalysisState) -> dict:
    """æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹ - æ±‡æ€»æ‰€æœ‰ç»“æœ"""
    print("ğŸ“‹ [æŠ¥å‘Šç”Ÿæˆ] æ±‡æ€»åˆ†æç»“æœ...")
    start_time = time.time()
    
    prompt = f"""åŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œç”Ÿæˆç»¼åˆæ•°æ®åˆ†ææŠ¥å‘Š:

æ•°æ®æ‘˜è¦:
{state.get('data_summary', 'æœªå®Œæˆ')}

å›¾è¡¨æè¿°:
{state.get('chart_description', 'æœªå®Œæˆ')}

æ•°æ®æ´å¯Ÿ:
{state.get('insights', 'æœªå®Œæˆ')}

è¦æ±‚:
1. æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
2. å½¢æˆé€»è¾‘æ¸…æ™°çš„æŠ¥å‘Šç»“æ„
3. æä¾›æ‰§è¡Œå»ºè®®
4. åŒ…å«å…³é”®æŒ‡æ ‡å’Œå¯è§†åŒ–å»ºè®®

è¿”å›æ ¼å¼: å®Œæ•´çš„æ•°æ®åˆ†ææŠ¥å‘Š"""
    
    response = llm.invoke([{"role": "user", "content": prompt}])
    processing_time = time.time() - start_time
    
    print(f"   æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
    
    return {
        "report": response.content,
        "processing_time": {
            **state.get("processing_time", {}),
            "report_generation": processing_time
        }
    }

# 7. å¹¶è¡Œæ‰§è¡Œå‡½æ•°
def parallel_analysis_node(state: AnalysisState) -> dict:
    """å¹¶è¡Œåˆ†æèŠ‚ç‚¹ - åŒæ—¶æ‰§è¡Œå¤šä¸ªåˆ†æä»»åŠ¡"""
    print("âš¡ [å¹¶è¡Œåˆ†æ] å¯åŠ¨å¤šä»»åŠ¡å¹¶è¡Œå¤„ç†...")
    
    # æ¨¡æ‹Ÿå¹¶è¡Œæ‰§è¡Œ(åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨asyncioæˆ–çº¿ç¨‹æ± )
    start_time = time.time()
    
    # é¡ºåºæ‰§è¡Œæ¨¡æ‹Ÿå¹¶è¡Œ(ä¸ºäº†æ¼”ç¤ºæ¸…æ™°)
    results = {}
    
    # ä»»åŠ¡1: æ•°æ®æ‘˜è¦
    summary_result = data_summary_node(state)
    results.update(summary_result)
    
    # ä»»åŠ¡2: å›¾è¡¨æè¿°  
    chart_result = chart_description_node(state)
    results.update(chart_result)
    
    # ä»»åŠ¡3: æ´å¯Ÿç”Ÿæˆ
    insights_result = insights_generation_node(state)
    results.update(insights_result)
    
    total_time = time.time() - start_time
    print(f"âš¡ å¹¶è¡Œä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    # åˆå¹¶å¤„ç†æ—¶é—´
    processing_times = results.get("processing_time", {})
    processing_times["parallel_total"] = total_time
    results["processing_time"] = processing_times
    
    return results

# 8. æ„å»ºä¸»å·¥ä½œæµ
def create_analysis_workflow():
    """åˆ›å»ºæ•°æ®åˆ†æå·¥ä½œæµ"""
    
    workflow = StateGraph(AnalysisState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("preprocessing", data_preprocessing_node)
    workflow.add_node("parallel_analysis", parallel_analysis_node)  
    workflow.add_node("report_generation", report_generation_node)
    
    # è®¾ç½®æµç¨‹
    workflow.add_edge(START, "preprocessing")
    workflow.add_edge("preprocessing", "parallel_analysis")
    workflow.add_edge("parallel_analysis", "report_generation")
    workflow.add_edge("report_generation", END)
    
    return workflow.compile()

# 9. æ¼”ç¤ºå‡½æ•°
def run_data_analysis_demo():
    """è¿è¡Œæ•°æ®åˆ†ææ¼”ç¤º"""
    print("=" * 70)
    print("ğŸ“Š Challenge 3: å¹¶è¡Œå¤„ç†å’Œå­å›¾ - æ•°æ®åˆ†æç³»ç»Ÿ")
    print("=" * 70)
    print("åŠŸèƒ½ç‰¹æ€§:")
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†å­å›¾ - éªŒè¯ã€æ¸…æ´—ã€è½¬æ¢")
    print("âš¡ å¹¶è¡Œåˆ†æå¤„ç† - æ‘˜è¦ã€å›¾è¡¨ã€æ´å¯ŸåŒæ—¶è¿›è¡Œ")
    print("ğŸ“‹ ç»“æœæ±‡æ€»æ•´åˆ - ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š")
    print("â±ï¸  æ€§èƒ½ç›‘æ§ - è®°å½•å„é˜¶æ®µå¤„ç†æ—¶é—´")
    print("-" * 70)
    
    # é€‰æ‹©æ•°æ®æº
    print("\né€‰æ‹©è¦åˆ†æçš„æ•°æ®:")
    for i, (key, value) in enumerate(SAMPLE_DATA.items(), 1):
        print(f"{i}. {key}: {value[:50]}...")
    
    try:
        choice = input("\né€‰æ‹©æ•°æ®æº (1-3, æˆ–ç›´æ¥å›è½¦ä½¿ç”¨é”€å”®æ•°æ®): ").strip()
        
        if choice == "2":
            selected_data = SAMPLE_DATA["user_behavior"]
        elif choice == "3":
            selected_data = SAMPLE_DATA["market"]
        else:
            selected_data = SAMPLE_DATA["sales"]
        
        print(f"\nâœ… å·²é€‰æ‹©æ•°æ®æºï¼Œå¼€å§‹åˆ†æ...")
        print("=" * 50)
        
        # åˆ›å»ºå·¥ä½œæµ
        analyzer = create_analysis_workflow()
        
        # åˆå§‹çŠ¶æ€
        initial_state: AnalysisState = {
            "messages": [HumanMessage(content="å¼€å§‹æ•°æ®åˆ†æ")],
            "raw_data": selected_data,
            "data_summary": "",
            "chart_description": "",
            "insights": "",
            "report": "",
            "processing_time": {}
        }
        
        # æ‰§è¡Œåˆ†æ
        print("ğŸš€ å¯åŠ¨åˆ†æå·¥ä½œæµ...")
        total_start = time.time()
        
        result = analyzer.invoke(initial_state)
        
        total_time = time.time() - total_start
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*70)
        print("ğŸ“‹ åˆ†ææŠ¥å‘Š")
        print("="*70)
        print(result.get("report", "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"))
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        print("\n" + "="*70)
        print("â±ï¸  æ€§èƒ½ç»Ÿè®¡")
        print("="*70)
        processing_times = result.get("processing_time", {})
        
        for stage, duration in processing_times.items():
            print(f"{stage:20}: {duration:.2f}ç§’")
        
        print(f"{'æ€»å¤„ç†æ—¶é—´':20}: {total_time:.2f}ç§’")
        
        # è®¡ç®—æ€§èƒ½æå‡
        sequential_time = sum(processing_times.get(key, 0) 
                            for key in ["data_summary", "chart_description", "insights"])
        parallel_time = processing_times.get("parallel_total", sequential_time)
        
        if sequential_time > 0:
            speedup = sequential_time / parallel_time
            print(f"\nğŸš€ å¹¶è¡Œå¤„ç†æ€§èƒ½æå‡: {speedup:.2f}x")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ åˆ†æä¸­æ–­")
    except Exception as e:
        print(f"âŒ åˆ†æé”™è¯¯: {e}")

def demo_subgraph():
    """æ¼”ç¤ºå­å›¾åŠŸèƒ½"""
    print("\nğŸ”§ å­å›¾æ¼”ç¤º:")
    print("-" * 30)
    
    preprocessor = create_preprocessing_subgraph()
    
    test_task: TaskState = {
        "task_id": "demo_task",
        "input_data": "ç¤ºä¾‹æ•°æ®: é”€å”®è®°å½•, ç”¨æˆ·è¡Œä¸º, å¸‚åœºæ•°æ®",
        "result": "",
        "processing_time": 0.0,
        "status": "pending"
    }
    
    print("æ‰§è¡Œé¢„å¤„ç†å­å›¾...")
    result = preprocessor.invoke(test_task)
    
    print(f"ç»“æœ: {result['result']}")
    print(f"çŠ¶æ€: {result['status']}")

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ Challenge 3: å¹¶è¡Œå¤„ç†å’Œå­å›¾")
    
    # æ¼”ç¤ºå­å›¾
    demo_subgraph()
    
    # è¿è¡Œæ•°æ®åˆ†ææ¼”ç¤º
    run_data_analysis_demo()
