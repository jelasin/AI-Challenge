"""
Challenge 7: æµå¼å¤„ç†å’Œå®æ—¶Agent

å­¦ä¹ ç›®æ ‡:
- æŒæ¡LangGraphçš„æµå¼å¤„ç†èƒ½åŠ›
- å­¦ä¹ å®æ—¶çŠ¶æ€æ›´æ–°å’Œå“åº”
- å®ç°å¼‚æ­¥Agentæ“ä½œ
- æ„å»ºäº‹ä»¶é©±åŠ¨çš„Agentç³»ç»Ÿ

æ ¸å¿ƒæ¦‚å¿µ:
1. stream() - æµå¼æ‰§è¡ŒçŠ¶æ€å›¾
2. å®æ—¶çŠ¶æ€æ›´æ–° - çŠ¶æ€å˜åŒ–çš„å®æ—¶ä¼ é€’
3. å¼‚æ­¥å¤„ç† - éé˜»å¡çš„Agentæ“ä½œ
4. äº‹ä»¶æµç›‘æ§ - å®æ—¶ç›‘æ§AgentçŠ¶æ€å˜åŒ–
"""

import os
import asyncio
import json
import time
from datetime import datetime
from typing import TypedDict, Annotated, AsyncIterator, List, Dict, Any
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage
from langchain_core.tools import tool

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if not os.getenv("OPENAI_API_KEY"):
    print("âš ï¸  è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    exit(1)

# 1. å®šä¹‰çŠ¶æ€ç»“æ„
class StreamingAgentState(TypedDict):
    """æµå¼AgentçŠ¶æ€"""
    messages: Annotated[list, add_messages]
    current_task: str  # å½“å‰æ‰§è¡Œçš„ä»»åŠ¡
    progress: float  # ä»»åŠ¡è¿›åº¦ (0-1)
    status: str  # running, completed, error
    events: List[Dict[str, Any]]  # äº‹ä»¶æ—¥å¿—
    stream_data: List[str]  # æµå¼æ•°æ®ç¼“å†²åŒº
    real_time_metrics: Dict[str, Any]  # å®æ—¶ç›‘æ§æŒ‡æ ‡

# 2. å·¥å…·å®šä¹‰
@tool
def process_data_stream(data: str, batch_size: int = 10) -> str:
    """
    æ¨¡æ‹Ÿæµå¼æ•°æ®å¤„ç†
    
    Args:
        data: è¦å¤„ç†çš„æ•°æ®
        batch_size: æ‰¹å¤„ç†å¤§å°
    
    Returns:
        å¤„ç†ç»“æœ
    """
    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†å»¶è¿Ÿ
    time.sleep(0.1)
    
    processed_items = len(data.split()) // batch_size + 1
    return f"å·²å¤„ç† {processed_items} ä¸ªæ•°æ®æ‰¹æ¬¡ï¼Œæ€»è®¡ {len(data)} ä¸ªå­—ç¬¦"

@tool
def real_time_monitor(metric_name: str, value: float) -> str:
    """
    å®æ—¶ç›‘æ§æŒ‡æ ‡è®°å½•
    
    Args:
        metric_name: æŒ‡æ ‡åç§°
        value: æŒ‡æ ‡å€¼
    
    Returns:
        ç›‘æ§ç»“æœ
    """
    timestamp = datetime.now().strftime("%H:%M:%S")
    return f"[{timestamp}] {metric_name}: {value:.2f}"

# 3. åˆå§‹åŒ–LLMå’Œå·¥å…·
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    streaming=True  # å¯ç”¨æµå¼è¾“å‡º
)

tools = [process_data_stream, real_time_monitor]
llm_with_tools = llm.bind_tools(tools)

# 4. èŠ‚ç‚¹å‡½æ•°å®šä¹‰
def initialize_stream(state: StreamingAgentState) -> StreamingAgentState:
    """åˆå§‹åŒ–æµå¼å¤„ç†"""
    print("ğŸš€ åˆå§‹åŒ–æµå¼Agentç³»ç»Ÿ...")
    
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # æ·»åŠ åˆå§‹åŒ–äº‹ä»¶
    event = {
        "timestamp": current_time,
        "type": "initialization",
        "message": "æµå¼Agentç³»ç»Ÿå¯åŠ¨"
    }
    
    return {
        **state,
        "current_task": "åˆå§‹åŒ–ä¸­",
        "progress": 0.0,
        "status": "running",
        "events": [event],
        "stream_data": [],
        "real_time_metrics": {
            "start_time": current_time,
            "processed_items": 0,
            "error_count": 0
        }
    }

def stream_processor(state: StreamingAgentState) -> StreamingAgentState:
    """æµå¼æ•°æ®å¤„ç†èŠ‚ç‚¹"""
    print("ğŸ“Š å¼€å§‹æµå¼æ•°æ®å¤„ç†...")
    
    messages = state["messages"]
    last_message = messages[-1] if messages else None
    
    if not last_message or not isinstance(last_message, HumanMessage):
        return {
            **state,
            "status": "error",
            "current_task": "é”™è¯¯: ç¼ºå°‘ç”¨æˆ·è¾“å…¥"
        }
    
    user_input = last_message.content
    
    # æ¨¡æ‹Ÿæµå¼å¤„ç†è¿‡ç¨‹
    stream_chunks = []
    total_chunks = 5
    
    for i in range(total_chunks):
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        time.sleep(0.2)
        
        chunk = f"å¤„ç†å— {i+1}/{total_chunks}: {user_input[:20]}..."
        stream_chunks.append(chunk)
        
        # æ›´æ–°è¿›åº¦
        progress = (i + 1) / total_chunks
        
        # å®æ—¶äº‹ä»¶è®°å½•
        event = {
            "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
            "type": "processing",
            "chunk_id": i + 1,
            "progress": progress,
            "data": chunk
        }
        
        print(f"  âš¡ [{event['timestamp']}] è¿›åº¦: {progress*100:.1f}% - {chunk}")
    
    return {
        **state,
        "current_task": "æµå¼å¤„ç†",
        "progress": 1.0,
        "stream_data": stream_chunks,
        "real_time_metrics": {
            **state["real_time_metrics"],
            "processed_items": state["real_time_metrics"]["processed_items"] + len(stream_chunks)
        }
    }

def tool_caller_node(state: StreamingAgentState) -> StreamingAgentState:
    """å·¥å…·è°ƒç”¨èŠ‚ç‚¹"""
    print("ğŸ”§ æ‰§è¡Œå·¥å…·è°ƒç”¨...")
    
    messages = state["messages"]
    last_message = messages[-1] if messages else None
    
    if not last_message:
        return {
            **state,
            "current_task": "å·¥å…·æ‰§è¡Œ",
            "events": state["events"] + [{
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "type": "tool_execution",
                "tools_called": 0,
                "results": []
            }]
        }
    
    # åŸºäºæ¶ˆæ¯å†…å®¹æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
    user_content = str(last_message.content) if hasattr(last_message, 'content') else ""
    
    tool_results = []
    
    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†å·¥å…·è°ƒç”¨
    if "æ•°æ®" in user_content or "å¤„ç†" in user_content:
        result = process_data_stream.invoke({"data": user_content, "batch_size": 5})
        tool_results.append({
            "tool": "process_data_stream",
            "args": {"data": user_content, "batch_size": 5},
            "result": result
        })
        print(f"  ğŸ› ï¸  è°ƒç”¨å·¥å…·: process_data_stream")
        print(f"  ğŸ“Š ç»“æœ: {result}")
    
    # æ¨¡æ‹Ÿç›‘æ§å·¥å…·è°ƒç”¨
    if "ç›‘æ§" in user_content or "æ€§èƒ½" in user_content or "æŒ‡æ ‡" in user_content:
        # æå–æ•°å€¼è¿›è¡Œç›‘æ§
        import re
        numbers = re.findall(r'\d+\.?\d*', user_content)
        if numbers:
            value = float(numbers[0])
            result = real_time_monitor.invoke({"metric_name": "ç³»ç»ŸæŒ‡æ ‡", "value": value})
            tool_results.append({
                "tool": "real_time_monitor", 
                "args": {"metric_name": "ç³»ç»ŸæŒ‡æ ‡", "value": value},
                "result": result
            })
            print(f"  ğŸ› ï¸  è°ƒç”¨å·¥å…·: real_time_monitor")
            print(f"  ğŸ“Š ç»“æœ: {result}")
    
    # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç‰¹å®šå·¥å…·éœ€æ±‚ï¼Œæ‰§è¡Œé€šç”¨å¤„ç†
    if not tool_results:
        result = f"å·²åˆ†æè¾“å…¥å†…å®¹ï¼Œæ£€æµ‹åˆ° {len(user_content)} ä¸ªå­—ç¬¦çš„æ•°æ®"
        tool_results.append({
            "tool": "é€šç”¨åˆ†æ",
            "args": {"content": user_content[:50] + "..."},
            "result": result
        })
        print(f"  ğŸ› ï¸  æ‰§è¡Œé€šç”¨åˆ†æ")
        print(f"  ğŸ“Š ç»“æœ: {result}")
    
    # åˆ›å»ºAIå“åº”æ¶ˆæ¯
    tool_summary = f"æ‰§è¡Œäº† {len(tool_results)} ä¸ªå·¥å…·è°ƒç”¨:\n"
    for tr in tool_results:
        tool_summary += f"- {tr['tool']}: {tr['result']}\n"
    
    ai_response = AIMessage(content=tool_summary)
    
    # è®°å½•å·¥å…·è°ƒç”¨äº‹ä»¶
    event = {
        "timestamp": datetime.now().strftime("%H:%M:%S"),
        "type": "tool_execution",
        "tools_called": len(tool_results),
        "results": tool_results
    }
    
    return {
        **state,
        "messages": state["messages"] + [ai_response],
        "current_task": "å·¥å…·æ‰§è¡Œ",
        "events": state["events"] + [event]
    }

def real_time_monitor_node(state: StreamingAgentState) -> StreamingAgentState:
    """å®æ—¶ç›‘æ§èŠ‚ç‚¹"""
    print("ğŸ“Š æ›´æ–°å®æ—¶ç›‘æ§æŒ‡æ ‡...")
    
    # è®¡ç®—å®æ—¶æŒ‡æ ‡
    current_time = datetime.now()
    start_time_str = state["real_time_metrics"]["start_time"]
    start_time = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
    
    elapsed_time = (current_time - start_time).total_seconds()
    
    metrics = {
        **state["real_time_metrics"],
        "elapsed_time": elapsed_time,
        "processing_rate": state["real_time_metrics"]["processed_items"] / max(elapsed_time, 1),
        "current_status": state["status"],
        "memory_usage": len(str(state)) / 1024,  # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨KB
        "last_update": current_time.strftime("%H:%M:%S")
    }
    
    # æ‰“å°å®æ—¶æŒ‡æ ‡
    print(f"  ğŸ“ˆ å¤„ç†é€Ÿç‡: {metrics['processing_rate']:.2f} items/sec")
    print(f"  â±ï¸  è¿è¡Œæ—¶é—´: {metrics['elapsed_time']:.1f}s")
    print(f"  ğŸ’¾ å†…å­˜ä½¿ç”¨: {metrics['memory_usage']:.1f}KB")
    
    return {
        **state,
        "real_time_metrics": metrics,
        "current_task": "ç›‘æ§æ›´æ–°"
    }

def response_generator(state: StreamingAgentState) -> StreamingAgentState:
    """å“åº”ç”ŸæˆèŠ‚ç‚¹"""
    print("ğŸ’¬ ç”Ÿæˆæµå¼å“åº”...")
    
    # æ”¶é›†å¤„ç†ç»“æœ
    events = state["events"]
    metrics = state["real_time_metrics"]
    stream_data = state["stream_data"]
    
    # ç”Ÿæˆæ€»ç»“å“åº”
    summary = f"""
ğŸ¯ **æµå¼å¤„ç†å®ŒæˆæŠ¥å‘Š**

ğŸ“Š **å¤„ç†ç»Ÿè®¡**:
- æ€»å¤„ç†æ—¶é—´: {metrics.get('elapsed_time', 0):.1f}ç§’
- å¤„ç†é¡¹ç›®æ•°: {metrics.get('processed_items', 0)}
- å¤„ç†é€Ÿç‡: {metrics.get('processing_rate', 0):.2f} items/sec
- æµå¼æ•°æ®å—: {len(stream_data)}

ğŸ“ˆ **å®æ—¶æŒ‡æ ‡**:
- å†…å­˜ä½¿ç”¨: {metrics.get('memory_usage', 0):.1f}KB
- å½“å‰çŠ¶æ€: {state['status']}
- æœ€åæ›´æ–°: {metrics.get('last_update', 'N/A')}

ğŸ“ **äº‹ä»¶æ—¥å¿—**: {len(events)} ä¸ªäº‹ä»¶å·²è®°å½•

âœ… æ‰€æœ‰æµå¼å¤„ç†ä»»åŠ¡å·²å®Œæˆï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚
"""
    
    ai_response = AIMessage(content=summary)
    
    return {
        **state,
        "messages": state["messages"] + [ai_response],
        "current_task": "å·²å®Œæˆ",
        "status": "completed"
    }

# 5. è·¯ç”±å‡½æ•°
def should_continue(state: StreamingAgentState) -> str:
    """å†³å®šå·¥ä½œæµæ˜¯å¦ç»§ç»­"""
    if state["status"] == "error":
        return END
    elif state["progress"] < 1.0:
        return "stream_processor"
    else:
        return "tool_caller"

def after_tools(state: StreamingAgentState) -> str:
    """å·¥å…·è°ƒç”¨åçš„è·¯ç”±"""
    return "monitor"

# 6. æ„å»ºçŠ¶æ€å›¾
def build_streaming_graph():
    """æ„å»ºæµå¼å¤„ç†çŠ¶æ€å›¾"""
    
    workflow = StateGraph(StreamingAgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("initialize", initialize_stream)
    workflow.add_node("stream_processor", stream_processor)
    workflow.add_node("tool_caller", tool_caller_node)
    workflow.add_node("monitor", real_time_monitor_node)
    workflow.add_node("response", response_generator)
    
    # æ·»åŠ è¾¹
    workflow.add_edge(START, "initialize")
    workflow.add_conditional_edges(
        "initialize",
        should_continue
    )
    workflow.add_conditional_edges(
        "stream_processor", 
        should_continue
    )
    workflow.add_conditional_edges(
        "tool_caller",
        after_tools
    )
    workflow.add_edge("monitor", "response")
    workflow.add_edge("response", END)
    
    return workflow.compile()

# 7. æµå¼å¤„ç†æ¼”ç¤º
async def run_streaming_demo():
    """è¿è¡Œæµå¼å¤„ç†æ¼”ç¤º"""
    print("ğŸŒŠ å¯åŠ¨LangGraphæµå¼å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    app = build_streaming_graph()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "è¯·å¤„ç†è¿™æ‰¹é”€å”®æ•°æ®: åŒ—äº¬100ä¸‡, ä¸Šæµ·150ä¸‡, å¹¿å·80ä¸‡, æ·±åœ³120ä¸‡",
        "åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®: ç‚¹å‡»ç‡3.2%, è½¬åŒ–ç‡1.8%, ç•™å­˜ç‡65%",
        "ç›‘æ§ç³»ç»Ÿæ€§èƒ½: CPUä½¿ç”¨ç‡75%, å†…å­˜ä½¿ç”¨ç‡60%, ç½‘ç»œå»¶è¿Ÿ20ms"
    ]
    
    for i, user_input in enumerate(test_cases, 1):
        print(f"\nğŸ¯ æµ‹è¯•æ¡ˆä¾‹ {i}: {user_input}")
        print("-" * 40)
        
        # åˆå§‹çŠ¶æ€
        initial_state: StreamingAgentState = {
            "messages": [HumanMessage(content=user_input)],
            "current_task": "",
            "progress": 0.0,
            "status": "initializing",
            "events": [],
            "stream_data": [],
            "real_time_metrics": {}
        }
        
        # ä½¿ç”¨streamæ–¹æ³•è·å–å®æ—¶æ›´æ–°
        print("ğŸ”„ å¼€å§‹æµå¼æ‰§è¡Œ...")
        
        try:
            # æµå¼æ‰§è¡ŒçŠ¶æ€å›¾
            async for chunk in app.astream(initial_state):
                for node_name, node_output in chunk.items():
                    print(f"ğŸ“ èŠ‚ç‚¹ '{node_name}' æ‰§è¡Œå®Œæˆ")
                    print(f"   å½“å‰ä»»åŠ¡: {node_output.get('current_task', 'N/A')}")
                    print(f"   è¿›åº¦: {node_output.get('progress', 0)*100:.1f}%")
                    print(f"   çŠ¶æ€: {node_output.get('status', 'unknown')}")
                    
                    # æ˜¾ç¤ºæœ€æ–°äº‹ä»¶
                    events = node_output.get('events', [])
                    if events:
                        latest_event = events[-1]
                        print(f"   ğŸ“ æœ€æ–°äº‹ä»¶: {latest_event.get('type', 'unknown')} - {latest_event.get('message', latest_event.get('data', 'N/A'))}")
                    
                    print()
                    
                    # æ¨¡æ‹Ÿå®æ—¶å¤„ç†å»¶è¿Ÿ
                    await asyncio.sleep(0.1)
                    
        except Exception as e:
            print(f"âŒ æµå¼å¤„ç†å‡ºé”™: {e}")
        
        print(f"âœ… æµ‹è¯•æ¡ˆä¾‹ {i} å®Œæˆ\n")
        await asyncio.sleep(1)  # æ¡ˆä¾‹é—´é—´éš”

def run_interactive_demo():
    """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
    print("ğŸŒŠ LangGraphæµå¼å¤„ç†äº¤äº’æ¼”ç¤º")
    print("=" * 40)
    print("è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
    print("è¾“å…¥ 'stream' æŸ¥çœ‹æµå¼æ•ˆæœ")
    print()
    
    app = build_streaming_graph()
    
    while True:
        user_input = input("ğŸ’¬ è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚: ").strip()
        
        if user_input.lower() == 'quit':
            break
        elif user_input.lower() == 'stream':
            print("ğŸš€ è¿è¡Œæµå¼å¤„ç†æ¼”ç¤º...")
            asyncio.run(run_streaming_demo())
            continue
        elif not user_input:
            continue
        
        print(f"\nğŸ¯ å¤„ç†è¯·æ±‚: {user_input}")
        print("-" * 30)
        
        # åˆå§‹çŠ¶æ€
        initial_state: StreamingAgentState = {
            "messages": [HumanMessage(content=user_input)],
            "current_task": "",
            "progress": 0.0,
            "status": "initializing", 
            "events": [],
            "stream_data": [],
            "real_time_metrics": {}
        }
        
        try:
            # æ‰§è¡ŒçŠ¶æ€å›¾
            result = app.invoke(initial_state)
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            final_message = result["messages"][-1]
            if hasattr(final_message, 'content'):
                print("ğŸ“‹ å¤„ç†ç»“æœ:")
                print(final_message.content)
            
        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™: {e}")
        
        print("\n" + "="*50 + "\n")

# 8. ä¸»ç¨‹åº
if __name__ == "__main__":
    print("ğŸŒŠ LangGraph Challenge 7: æµå¼å¤„ç†å’Œå®æ—¶Agent")
    print("=" * 50)
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    mode = input("é€‰æ‹©è¿è¡Œæ¨¡å¼:\n1. äº¤äº’æ¨¡å¼\n2. æµå¼æ¼”ç¤º\nè¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
    
    if mode == "2":
        print("\nğŸš€ å¯åŠ¨æµå¼æ¼”ç¤ºæ¨¡å¼...")
        asyncio.run(run_streaming_demo())
    else:
        print("\nğŸš€ å¯åŠ¨äº¤äº’æ¨¡å¼...")
        run_interactive_demo()
    
    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ LangGraph æµå¼å¤„ç†ç³»ç»Ÿ!")
