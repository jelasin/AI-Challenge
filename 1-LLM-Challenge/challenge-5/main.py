# -*- coding: utf-8 -*-
"""
Challenge 5: LCEL (LangChain Expression Language) å’Œé“¾ç»„åˆ
éš¾åº¦ï¼šä¸­çº§åˆ°é«˜çº§

å­¦ä¹ ç›®æ ‡ï¼š
1. æŒæ¡LCELè¯­æ³•å’Œæ ¸å¿ƒæ¦‚å¿µ
2. å­¦ä¹ Runnableæ¥å£çš„ä½¿ç”¨
3. å®ç°å¤æ‚é“¾çš„ç»„åˆå’Œåˆ†æ”¯
4. æŒæ¡å¹¶è¡Œå¤„ç†å’Œè·¯ç”±
5. å­¦ä¹ æµå¼å¤„ç†å’Œå¼‚æ­¥æ“ä½œ
6. å®ç°é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶

ä»»åŠ¡æè¿°ï¼š
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½å†…å®¹å¤„ç†ç®¡é“ï¼Œèƒ½å¤Ÿï¼š
1. å¹¶è¡Œå¤„ç†å¤šç§å†…å®¹åˆ†æä»»åŠ¡
2. æ ¹æ®å†…å®¹ç±»å‹åŠ¨æ€è·¯ç”±
3. å®ç°æµå¼è¾“å‡º
4. æ·»åŠ é”™è¯¯å¤„ç†å’Œå›é€€
5. æ”¯æŒé…ç½®åŒ–å’Œå¯æ‰©å±•æ€§
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser
from langchain_core.runnables import (
    RunnablePassthrough, 
    RunnableParallel, 
    RunnableLambda,
    RunnableBranch,
    Runnable
)
from langchain_core.runnables.config import RunnableConfig
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Optional, Union, Literal
import asyncio
import json
import os
import time

class ContentAnalysis(BaseModel):
    """å†…å®¹åˆ†æç»“æœæ¨¡å‹"""
    content_type: Literal["article", "code", "data", "unknown"] = Field(description="å†…å®¹ç±»å‹")
    language: Optional[str] = Field(description="è¯­è¨€ï¼ˆå¦‚æœæ˜¯ä»£ç æˆ–æ–‡æœ¬ï¼‰")
    sentiment: Optional[Literal["positive", "negative", "neutral"]] = Field(description="æƒ…æ„Ÿå€¾å‘")
    complexity: Literal["low", "medium", "high"] = Field(description="å¤æ‚åº¦")
    key_topics: List[str] = Field(description="å…³é”®ä¸»é¢˜")
    summary: str = Field(description="å†…å®¹æ‘˜è¦")
    confidence: float = Field(description="åˆ†æç½®ä¿¡åº¦", ge=0.0, le=1.0)

class ProcessingResult(BaseModel):
    """å¤„ç†ç»“æœæ¨¡å‹"""
    analysis: ContentAnalysis
    suggestions: List[str]
    metadata: Dict[str, Any]
    processing_time: float

def create_content_classifier():
    """åˆ›å»ºå†…å®¹åˆ†ç±»å™¨"""
    prompt = ChatPromptTemplate.from_template("""
    åˆ†æä»¥ä¸‹å†…å®¹å¹¶ç¡®å®šå…¶ç±»å‹ï¼š

    å†…å®¹ï¼š{content}

    è¯·åˆ¤æ–­è¿™æ˜¯ï¼š
    - article: æ–‡ç« æˆ–æ–‡æ¡£
    - code: ä»£ç ç‰‡æ®µ
    - data: æ•°æ®æˆ–è¡¨æ ¼
    - unknown: æ— æ³•ç¡®å®š

    åªè¿”å›ç±»å‹åç§°ï¼Œä¸è¦è§£é‡Šã€‚
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    return prompt | llm | StrOutputParser()

def create_sentiment_analyzer():
    """åˆ›å»ºæƒ…æ„Ÿåˆ†æå™¨"""
    prompt = ChatPromptTemplate.from_template("""
    åˆ†æä»¥ä¸‹å†…å®¹çš„æƒ…æ„Ÿå€¾å‘ï¼š

    å†…å®¹ï¼š{content}

    è¯·åˆ¤æ–­æƒ…æ„Ÿå€¾å‘ï¼špositive, negative, æˆ– neutral
    åªè¿”å›ä¸€ä¸ªè¯ï¼Œä¸è¦è§£é‡Šã€‚
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    return prompt | llm | StrOutputParser()

def create_summarizer():
    """åˆ›å»ºæ‘˜è¦ç”Ÿæˆå™¨"""
    prompt = ChatPromptTemplate.from_template("""
    è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡100å­—ï¼‰ï¼š

    å†…å®¹ï¼š{content}

    æ‘˜è¦ï¼š
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
    
    return prompt | llm | StrOutputParser()

def create_topic_extractor():
    """åˆ›å»ºä¸»é¢˜æå–å™¨"""
    prompt = ChatPromptTemplate.from_template("""
    ä»ä»¥ä¸‹å†…å®¹ä¸­æå–3-5ä¸ªå…³é”®ä¸»é¢˜æˆ–å…³é”®è¯ï¼š

    å†…å®¹ï¼š{content}

    è¯·ä»¥JSONæ ¼å¼è¿”å›ä¸»é¢˜åˆ—è¡¨ï¼š
    {{"topics": ["ä¸»é¢˜1", "ä¸»é¢˜2", "ä¸»é¢˜3"]}}
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
    
    return prompt | llm | JsonOutputParser()

def create_complexity_analyzer():
    """åˆ›å»ºå¤æ‚åº¦åˆ†æå™¨"""
    prompt = ChatPromptTemplate.from_template("""
    è¯„ä¼°ä»¥ä¸‹å†…å®¹çš„å¤æ‚åº¦ï¼š

    å†…å®¹ï¼š{content}

    å¤æ‚åº¦æ ‡å‡†ï¼š
    - low: ç®€å•æ˜“æ‡‚ï¼ŒåŸºç¡€æ¦‚å¿µ
    - medium: éœ€è¦ä¸€å®šä¸“ä¸šçŸ¥è¯†
    - high: å¤æ‚æ¦‚å¿µï¼Œéœ€è¦æ·±åº¦ä¸“ä¸šçŸ¥è¯†

    åªè¿”å›å¤æ‚åº¦ç­‰çº§ï¼šlow, medium, æˆ– high
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    return prompt | llm | StrOutputParser()

def create_language_detector():
    """åˆ›å»ºè¯­è¨€æ£€æµ‹å™¨ï¼ˆé’ˆå¯¹ä»£ç ï¼‰"""
    def detect_language(content: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹é€»è¾‘"""
        content_lower = content.lower()
        
        # æ£€æµ‹å¸¸è§çš„ä»£ç ç‰¹å¾
        if any(keyword in content_lower for keyword in ['def ', 'import ', 'from ', 'print(']):
            return "python"
        elif any(keyword in content_lower for keyword in ['function', 'var ', 'let ', 'const ']):
            return "javascript"  
        elif any(keyword in content_lower for keyword in ['public class', 'private ', 'static void']):
            return "java"
        elif any(keyword in content_lower for keyword in ['#include', 'int main', 'printf']):
            return "c"
        elif any(keyword in content_lower for keyword in ['SELECT', 'FROM', 'WHERE', 'INSERT']):
            return "sql"
        else:
            return "unknown"
    
    return RunnableLambda(lambda x: detect_language(x.get("content", "") if isinstance(x, dict) else str(x)))

def demo_basic_lcel():
    """æ¼”ç¤ºåŸºæœ¬çš„LCELè¯­æ³•"""
    print("ğŸ”— LCELåŸºç¡€è¯­æ³•æ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # 1. ç®€å•çš„é“¾ç»„åˆ
    print("1. ç®€å•é“¾ç»„åˆ (prompt | llm | parser):")
    
    prompt = ChatPromptTemplate.from_template("å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    parser = StrOutputParser()
    
    # ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ç»„åˆ
    simple_chain = prompt | llm | parser
    
    result = simple_chain.invoke({"text": "ä½ å¥½ï¼Œä¸–ç•Œï¼"})
    print(f"   è¾“å…¥: ä½ å¥½ï¼Œä¸–ç•Œï¼")
    print(f"   è¾“å‡º: {result}")
    
    # 2. ä½¿ç”¨RunnablePassthrough
    print("\n2. ä½¿ç”¨RunnablePassthroughä¿æŒè¾“å…¥:")
    
    passthrough_chain = RunnableParallel({
        "original": RunnablePassthrough(),
        "translation": prompt | llm | parser
    })
    
    result = passthrough_chain.invoke({"text": "æ—©ä¸Šå¥½"})
    print(f"   ç»“æœ: {result}")
    
    # 3. ä½¿ç”¨RunnableParallelå¹¶è¡Œå¤„ç†
    print("\n3. å¹¶è¡Œå¤„ç†æ¼”ç¤º:")
    
    parallel_chain = RunnableParallel({
        "english": ChatPromptTemplate.from_template("Translate to English: {text}") | llm | parser,
        "summary": ChatPromptTemplate.from_template("Summarize in Chinese: {text}") | llm | parser,
        "keywords": ChatPromptTemplate.from_template("Extract 3 keywords from: {text}") | llm | parser
    })
    
    result = parallel_chain.invoke({"text": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"})
    print(f"   è‹±æ–‡ç¿»è¯‘: {result['english'][:50]}...")
    print(f"   ä¸­æ–‡æ‘˜è¦: {result['summary'][:50]}...")
    print(f"   å…³é”®è¯: {result['keywords'][:50]}...")

def create_content_router():
    """åˆ›å»ºå†…å®¹è·¯ç”±å™¨"""
    
    def route_content(content_info: Dict[str, Any]) -> Runnable:
        """æ ¹æ®å†…å®¹ç±»å‹è·¯ç”±åˆ°ä¸åŒçš„å¤„ç†é“¾"""
        content_type = content_info.get("content_type", "unknown")
        
        if content_type == "code":
            return create_code_processor()
        elif content_type == "article":
            return create_article_processor()
        elif content_type == "data":
            return create_data_processor()
        else:
            return create_default_processor()
    
    return RunnableLambda(route_content)

def create_code_processor():
    """åˆ›å»ºä»£ç å¤„ç†å™¨"""
    prompt = ChatPromptTemplate.from_template("""
    åˆ†æä»¥ä¸‹ä»£ç ï¼š

    ä»£ç ï¼š{content}
    è¯­è¨€ï¼š{language}

    è¯·æä¾›ï¼š
    1. ä»£ç åŠŸèƒ½è¯´æ˜
    2. ä»£ç è´¨é‡è¯„ä¼°
    3. ä¼˜åŒ–å»ºè®®

    ä»¥JSONæ ¼å¼è¿”å›ï¼š
    {{
        "functionality": "åŠŸèƒ½è¯´æ˜",
        "quality_score": "è¯„åˆ†(1-10)",
        "suggestions": ["å»ºè®®1", "å»ºè®®2"]
    }}
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
    
    return prompt | llm | JsonOutputParser()

def create_article_processor():
    """åˆ›å»ºæ–‡ç« å¤„ç†å™¨"""
    prompt = ChatPromptTemplate.from_template("""
    åˆ†æä»¥ä¸‹æ–‡ç« ï¼š

    æ–‡ç« ï¼š{content}

    è¯·æä¾›ï¼š
    1. æ–‡ç« ä¸»è¦è§‚ç‚¹
    2. å†™ä½œè´¨é‡è¯„ä¼°
    3. æ”¹è¿›å»ºè®®

    ä»¥JSONæ ¼å¼è¿”å›ï¼š
    {{
        "main_points": ["è§‚ç‚¹1", "è§‚ç‚¹2"],
        "writing_quality": "è´¨é‡è¯„ä¼°",
        "suggestions": ["å»ºè®®1", "å»ºè®®2"]
    }}
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
    
    return prompt | llm | JsonOutputParser()

def create_data_processor():
    """åˆ›å»ºæ•°æ®å¤„ç†å™¨"""
    prompt = ChatPromptTemplate.from_template("""
    åˆ†æä»¥ä¸‹æ•°æ®ï¼š

    æ•°æ®ï¼š{content}

    è¯·æä¾›ï¼š
    1. æ•°æ®ç»“æ„åˆ†æ
    2. æ•°æ®è´¨é‡è¯„ä¼°
    3. åˆ†æå»ºè®®

    ä»¥JSONæ ¼å¼è¿”å›ï¼š
    {{
        "structure": "ç»“æ„æè¿°",
        "quality": "è´¨é‡è¯„ä¼°", 
        "suggestions": ["å»ºè®®1", "å»ºè®®2"]
    }}
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
    
    return prompt | llm | JsonOutputParser()

def create_default_processor():
    """åˆ›å»ºé»˜è®¤å¤„ç†å™¨"""
    prompt = ChatPromptTemplate.from_template("""
    åˆ†æä»¥ä¸‹å†…å®¹ï¼š

    å†…å®¹ï¼š{content}

    è¯·æä¾›é€šç”¨çš„å†…å®¹åˆ†æå’Œå»ºè®®ã€‚

    ä»¥JSONæ ¼å¼è¿”å›ï¼š
    {{
        "analysis": "åˆ†æç»“æœ",
        "suggestions": ["å»ºè®®1", "å»ºè®®2"]
    }}
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
    
    return prompt | llm | JsonOutputParser()

def create_advanced_content_pipeline():
    """åˆ›å»ºé«˜çº§å†…å®¹å¤„ç†ç®¡é“"""
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    
    print("ğŸ—ï¸ æ„å»ºé«˜çº§å†…å®¹å¤„ç†ç®¡é“...")
    
    # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶è¡Œåˆ†æ
    analysis_stage = RunnableParallel({
        "content_type": create_content_classifier(),
        "sentiment": create_sentiment_analyzer(), 
        "summary": create_summarizer(),
        "topics": create_topic_extractor(),
        "complexity": create_complexity_analyzer(),
        "language": create_language_detector(),
        "original": RunnablePassthrough()
    })
    
    # ç¬¬äºŒé˜¶æ®µï¼šè·¯ç”±å¤„ç†
    def create_routing_logic():
        """åˆ›å»ºè·¯ç”±é€»è¾‘"""
        def route_and_process(analysis_result: Dict[str, Any]) -> Dict[str, Any]:
            content_type = analysis_result.get("content_type", "unknown").strip()
            content = analysis_result["original"]["content"]
            
            # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©å¤„ç†å™¨
            if "code" in content_type.lower():
                processor = create_code_processor()
            elif "article" in content_type.lower():
                processor = create_article_processor()
            elif "data" in content_type.lower():
                processor = create_data_processor()
            else:
                processor = create_default_processor()
            
            # å¤„ç†å†…å®¹
            try:
                processing_result = processor.invoke({
                    "content": content,
                    "language": analysis_result.get("language", "unknown")
                })
            except Exception as e:
                processing_result = {
                    "error": f"å¤„ç†å¤±è´¥: {str(e)}",
                    "suggestions": ["è¯·æ£€æŸ¥å†…å®¹æ ¼å¼", "å°è¯•é‡æ–°æäº¤"]
                }
            
            # åˆå¹¶ç»“æœ
            return {
                "analysis": ContentAnalysis(
                    content_type=content_type if content_type in ["article", "code", "data"] else "unknown",
                    language=analysis_result.get("language"),
                    sentiment=analysis_result.get("sentiment"),
                    complexity=analysis_result.get("complexity", "medium"),
                    key_topics=analysis_result.get("topics", {}).get("topics", []),
                    summary=analysis_result.get("summary", ""),
                    confidence=0.8  # ç®€åŒ–çš„ç½®ä¿¡åº¦
                ),
                "processing_result": processing_result,
                "metadata": {
                    "processing_time": time.time(),
                    "content_length": len(content)
                }
            }
        
        return RunnableLambda(route_and_process)
    
    # ç»„åˆå®Œæ•´ç®¡é“
    complete_pipeline = analysis_stage | create_routing_logic()
    
    return complete_pipeline

def demo_streaming():
    """æ¼”ç¤ºæµå¼å¤„ç†"""
    print("\nğŸŒŠ æµå¼å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºæ”¯æŒæµå¼è¾“å‡ºçš„é“¾
    prompt = ChatPromptTemplate.from_template("""
    è¯·è¯¦ç»†è§£é‡Šä»¥ä¸‹æ¦‚å¿µï¼ŒåŒ…æ‹¬å®šä¹‰ã€ç‰¹ç‚¹ã€åº”ç”¨åœºæ™¯ç­‰ï¼š

    æ¦‚å¿µï¼š{concept}
    """)
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0.3, streaming=True)
    chain = prompt | llm | StrOutputParser()
    
    print("æµå¼è¾“å‡ºç¤ºä¾‹ï¼ˆæ¦‚å¿µè§£é‡Šï¼‰:")
    print("-" * 30)
    
    # æµå¼å¤„ç†
    for chunk in chain.stream({"concept": "æœºå™¨å­¦ä¹ "}):
        print(chunk, end="", flush=True)
    
    print("\n")

async def demo_async_processing():
    """æ¼”ç¤ºå¼‚æ­¥å¤„ç†"""
    print("\nâš¡ å¼‚æ­¥å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºå¼‚æ­¥é“¾
    prompt = ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼š{topic}")
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
    chain = prompt | llm | StrOutputParser()
    
    # å‡†å¤‡å¤šä¸ªä»»åŠ¡
    topics = ["äººå·¥æ™ºèƒ½", "åŒºå—é“¾", "é‡å­è®¡ç®—", "ç”Ÿç‰©æŠ€æœ¯", "æ–°èƒ½æº"]
    
    print("å¹¶å‘å¤„ç†å¤šä¸ªä¸»é¢˜...")
    start_time = time.time()
    
    # å¹¶å‘æ‰§è¡Œ
    tasks = [chain.ainvoke({"topic": topic}) for topic in topics]
    results = await asyncio.gather(*tasks)
    
    end_time = time.time()
    
    print(f"å¤„ç†å®Œæˆï¼Œç”¨æ—¶ï¼š{end_time - start_time:.2f}ç§’")
    for topic, result in zip(topics, results):
        print(f"  {topic}: {result}")

def demo_error_handling():
    """æ¼”ç¤ºé”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶"""
    print("\nğŸ›¡ï¸ é”™è¯¯å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºå¯èƒ½å¤±è´¥çš„é“¾
    def failing_function(x):
        if "error" in x["text"].lower():
            raise ValueError("æ•…æ„è§¦å‘çš„é”™è¯¯")
        return x["text"].upper()
    
    # åˆ›å»ºå›é€€é“¾
    def fallback_function(x):
        return f"å›é€€å¤„ç†: {x['text']}"
    
    # ä½¿ç”¨try-exceptåŒ…è£…çš„é“¾
    def safe_processing(x):
        try:
            return failing_function(x)
        except Exception as e:
            print(f"  âš ï¸ æ•è·é”™è¯¯: {e}")
            return fallback_function(x)
    
    safe_chain = RunnableLambda(safe_processing)
    
    # æµ‹è¯•æ­£å¸¸æƒ…å†µ
    print("1. æ­£å¸¸å¤„ç†:")
    result = safe_chain.invoke({"text": "hello world"})
    print(f"   ç»“æœ: {result}")
    
    # æµ‹è¯•é”™è¯¯æƒ…å†µ
    print("\n2. é”™è¯¯å¤„ç†:")
    result = safe_chain.invoke({"text": "this will cause an error"})
    print(f"   ç»“æœ: {result}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ”— LangChain Challenge 5: LCELå’Œé“¾ç»„åˆ")
        print("=" * 60)
        
        # åŸºç¡€LCELæ¼”ç¤º
        demo_basic_lcel()
        
        # æµå¼å¤„ç†æ¼”ç¤º
        demo_streaming()
        
        # é”™è¯¯å¤„ç†æ¼”ç¤º
        demo_error_handling()
        
        print("\n" + "=" * 60)
        print("ğŸ—ï¸ æ„å»ºå’Œæµ‹è¯•é«˜çº§å†…å®¹å¤„ç†ç®¡é“...")
        
        # åˆ›å»ºé«˜çº§ç®¡é“
        pipeline = create_advanced_content_pipeline()
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„å†…å®¹
        test_contents = [
            {
                "content": """
                def fibonacci(n):
                    if n <= 1:
                        return n
                    return fibonacci(n-1) + fibonacci(n-2)
                """,
                "description": "Pythonä»£ç "
            },
            {
                "content": """
                äººå·¥æ™ºèƒ½çš„å‘å±•æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚ä»è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ï¼Œ
                AIæŠ€æœ¯å·²ç»æ·±å…¥åˆ°æˆ‘ä»¬ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚ç„¶è€Œï¼Œè¿™ç§å¿«é€Ÿå‘å±•ä¹Ÿå¸¦æ¥äº†
                æ–°çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚æˆ‘ä»¬éœ€è¦è°¨æ…åœ°å¹³è¡¡æŠ€æœ¯è¿›æ­¥ä¸ä¼¦ç†è€ƒé‡ï¼Œç¡®ä¿AI
                çš„å‘å±•èƒ½å¤Ÿé€ ç¦å…¨äººç±»ã€‚
                """,
                "description": "ä¸­æ–‡æ–‡ç« "
            },
            {
                "content": """
                Name,Age,Department,Salary
                Alice,25,Engineering,75000
                Bob,30,Marketing,65000
                Carol,28,Sales,70000
                David,35,Engineering,85000
                """,
                "description": "CSVæ•°æ®"
            }
        ]
        
        for i, test_case in enumerate(test_contents, 1):
            print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['description']}")
            print("-" * 40)
            
            start_time = time.time()
            result = pipeline.invoke({"content": test_case["content"]})
            end_time = time.time()
            
            analysis = result["analysis"]
            processing_result = result["processing_result"]
            
            print(f"ğŸ” åˆ†æç»“æœ:")
            print(f"   ç±»å‹: {analysis.content_type}")
            print(f"   è¯­è¨€: {analysis.language}")
            print(f"   æƒ…æ„Ÿ: {analysis.sentiment}")
            print(f"   å¤æ‚åº¦: {analysis.complexity}")
            print(f"   å…³é”®ä¸»é¢˜: {', '.join(analysis.key_topics[:3])}")
            print(f"   æ‘˜è¦: {analysis.summary[:100]}...")
            print(f"   ç½®ä¿¡åº¦: {analysis.confidence:.2f}")
            
            print(f"\nâš™ï¸ å¤„ç†ç»“æœ:")
            if isinstance(processing_result, dict):
                for key, value in processing_result.items():
                    if key != "suggestions":
                        print(f"   {key}: {str(value)[:80]}...")
            
            print(f"\nâ±ï¸ å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
        
        print("\n" + "=" * 60)
        print("âš¡ å¼‚æ­¥å¤„ç†æ¼”ç¤º...")
        
        # å¼‚æ­¥å¤„ç†æ¼”ç¤º
        asyncio.run(demo_async_processing())
        
        print("\n" + "=" * 60)
        print("ğŸ¯ ç»ƒä¹ ä»»åŠ¡:")
        print("1. å®ç°æ¡ä»¶åˆ†æ”¯è·¯ç”± (RunnableBranch)")
        print("2. æ·»åŠ é“¾çš„é…ç½®åŒ–åŠŸèƒ½ (RunnableConfig)")
        print("3. å®ç°è‡ªå®šä¹‰Runnableç±»")
        print("4. æ·»åŠ é“¾çš„ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½")
        print("5. å®ç°é“¾çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–")
        print("6. æ·»åŠ æ›´å¤æ‚çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        print("7. å®ç°åŠ¨æ€é“¾ç»„åˆå’Œæ’ä»¶ç³»ç»Ÿ")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("1. å·²è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("2. å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…: pip install langchain langchain-openai")

if __name__ == "__main__":
    main()
