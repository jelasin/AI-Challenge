# -*- coding: utf-8 -*-
"""
Challenge 8: ç»¼åˆé¡¹ç›® - æ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
éš¾åº¦ï¼šä¸“å®¶çº§

å­¦ä¹ ç›®æ ‡ï¼š
1. ç»¼åˆè¿ç”¨æ‰€æœ‰LangChainç‰¹æ€§
2. æ„å»ºå®Œæ•´çš„ä¼ä¸šçº§AIåº”ç”¨
3. å®ç°å¤æ‚çš„å¤šæ¨¡æ€å¤„ç†
4. é›†æˆå¤šç§æ•°æ®æºå’ŒæœåŠ¡
5. å®ç°é«˜å¯ç”¨æ€§å’Œå¯æ‰©å±•æ€§
6. æ·»åŠ ç›‘æ§ã€æ—¥å¿—å’Œå®‰å…¨æ§åˆ¶

é¡¹ç›®æè¿°ï¼š
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿï¼Œæ•´åˆï¼š
- å¤šæºæ–‡æ¡£å¤„ç†å’ŒRAG
- æ™ºèƒ½Agentå’Œå·¥å…·è°ƒç”¨
- å®æ—¶é—®ç­”å’Œæ¨ç†
- çŸ¥è¯†å›¾è°±æ„å»º
- å¤šç”¨æˆ·åä½œ
- APIæœåŠ¡å’ŒWebç•Œé¢
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool, tool
from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.callbacks import BaseCallbackHandler
from langchain_community.document_loaders import TextLoader, DirectoryLoader, CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.memory import ConversationBufferMemory

from pydantic import BaseModel, Field
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime, timedelta
import asyncio
import json
import os
import sqlite3
import hashlib
import logging
from dataclasses import dataclass, asdict
from pathlib import Path
import tempfile
import shutil

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =========================== æ•°æ®æ¨¡å‹ ===========================

class DocumentMetadata(BaseModel):
    """æ–‡æ¡£å…ƒæ•°æ®æ¨¡å‹"""
    id: str
    title: str
    author: Optional[str] = None
    created_date: datetime
    modified_date: datetime
    file_path: str
    file_size: int
    content_hash: str
    tags: List[str] = []
    category: Optional[str] = None
    summary: Optional[str] = None
    language: str = "zh-CN"

class KnowledgeEntity(BaseModel):
    """çŸ¥è¯†å®ä½“æ¨¡å‹"""
    id: str
    name: str
    entity_type: str  # person, organization, concept, technology
    description: str
    confidence: float
    source_documents: List[str]
    related_entities: List[str] = []

class UserSession(BaseModel):
    """ç”¨æˆ·ä¼šè¯æ¨¡å‹"""
    session_id: str
    user_id: str
    created_at: datetime
    last_activity: datetime
    conversation_history: List[Dict[str, Any]] = []
    preferences: Dict[str, Any] = {}

class QueryResult(BaseModel):
    """æŸ¥è¯¢ç»“æœæ¨¡å‹"""
    query: str
    answer: str
    confidence: float
    sources: List[str]
    related_entities: List[str]
    suggestions: List[str]
    processing_time: float

# =========================== æ•°æ®åº“ç®¡ç† ===========================

class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "knowledge_system.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæ–‡æ¡£è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                author TEXT,
                created_date TIMESTAMP,
                modified_date TIMESTAMP,
                file_path TEXT,
                file_size INTEGER,
                content_hash TEXT,
                tags TEXT,
                category TEXT,
                summary TEXT,
                language TEXT
            )
        ''')
        
        # åˆ›å»ºçŸ¥è¯†å®ä½“è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS entities (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                entity_type TEXT,
                description TEXT,
                confidence REAL,
                source_documents TEXT,
                related_entities TEXT
            )
        ''')
        
        # åˆ›å»ºç”¨æˆ·ä¼šè¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS sessions (
                session_id TEXT PRIMARY KEY,
                user_id TEXT,
                created_at TIMESTAMP,
                last_activity TIMESTAMP,
                conversation_history TEXT,
                preferences TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def save_document(self, doc: DocumentMetadata):
        """ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO documents 
            (id, title, author, created_date, modified_date, file_path, 
             file_size, content_hash, tags, category, summary, language)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            doc.id, doc.title, doc.author, doc.created_date, doc.modified_date,
            doc.file_path, doc.file_size, doc.content_hash, 
            json.dumps(doc.tags), doc.category, doc.summary, doc.language
        ))
        
        conn.commit()
        conn.close()
    
    def get_document(self, doc_id: str) -> Optional[DocumentMetadata]:
        """è·å–æ–‡æ¡£å…ƒæ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM documents WHERE id = ?', (doc_id,))
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return DocumentMetadata(
                id=row[0], title=row[1], author=row[2],
                created_date=datetime.fromisoformat(row[3]),
                modified_date=datetime.fromisoformat(row[4]),
                file_path=row[5], file_size=row[6], content_hash=row[7],
                tags=json.loads(row[8]) if row[8] else [],
                category=row[9], summary=row[10], language=row[11]
            )
        return None
    
    def save_entity(self, entity: KnowledgeEntity):
        """ä¿å­˜çŸ¥è¯†å®ä½“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO entities 
            (id, name, entity_type, description, confidence, source_documents, related_entities)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            entity.id, entity.name, entity.entity_type, entity.description,
            entity.confidence, json.dumps(entity.source_documents),
            json.dumps(entity.related_entities)
        ))
        
        conn.commit()
        conn.close()

# =========================== æ–‡æ¡£å¤„ç†å™¨ ===========================

class DocumentProcessor:
    """æ™ºèƒ½æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\\n\\n", "\\n", "ã€‚", "ï¼", "ï¼Ÿ", " "]
        )
        
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨ç›®å½•
        self.vector_store_path = "vector_stores"
        os.makedirs(self.vector_store_path, exist_ok=True)
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = None
        self.load_or_create_vector_store()
    
    def load_or_create_vector_store(self):
        """åŠ è½½æˆ–åˆ›å»ºå‘é‡å­˜å‚¨"""
        vector_store_file = os.path.join(self.vector_store_path, "main_index")
        
        if os.path.exists(vector_store_file + ".faiss"):
            logger.info("åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨")
            self.vector_store = FAISS.load_local(
                vector_store_file, 
                self.embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            logger.info("åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„å‘é‡å­˜å‚¨
            from langchain_core.documents import Document
            dummy_doc = Document(page_content="åˆå§‹åŒ–æ–‡æ¡£", metadata={"source": "init"})
            self.vector_store = FAISS.from_documents([dummy_doc], self.embeddings)
            self.save_vector_store()
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡å­˜å‚¨"""
        if self.vector_store:
            vector_store_file = os.path.join(self.vector_store_path, "main_index")
            self.vector_store.save_local(vector_store_file)
            logger.info("å‘é‡å­˜å‚¨å·²ä¿å­˜")
    
    def calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()
    
    def extract_metadata(self, file_path: str, content: str) -> DocumentMetadata:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        file_stat = os.stat(file_path)
        content_hash = self.calculate_content_hash(content)
        
        # ç”Ÿæˆæ ‡é¢˜ï¼ˆä»å†…å®¹çš„å‰100ä¸ªå­—ç¬¦æˆ–æ–‡ä»¶åï¼‰
        title = content[:100].strip() if content else os.path.basename(file_path)
        if len(title) > 50:
            title = title[:50] + "..."
        
        doc_id = hashlib.md5(file_path.encode()).hexdigest()
        
        return DocumentMetadata(
            id=doc_id,
            title=title,
            created_date=datetime.fromtimestamp(file_stat.st_ctime),
            modified_date=datetime.fromtimestamp(file_stat.st_mtime),
            file_path=file_path,
            file_size=file_stat.st_size,
            content_hash=content_hash
        )
    
    def generate_summary(self, content: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        if len(content) < 200:
            return content
        
        prompt = ChatPromptTemplate.from_template(
            "è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š\\n\\n{content}"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            summary = chain.invoke({"content": content[:2000]})  # é™åˆ¶è¾“å…¥é•¿åº¦
            return summary
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return content[:200] + "..."
    
    def extract_entities(self, content: str, doc_id: str) -> List[KnowledgeEntity]:
        """æå–çŸ¥è¯†å®ä½“"""
        prompt = ChatPromptTemplate.from_template("""
        ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–é‡è¦çš„çŸ¥è¯†å®ä½“ï¼ŒåŒ…æ‹¬äººåã€ç»„ç»‡ã€æ¦‚å¿µã€æŠ€æœ¯ç­‰ã€‚

        æ–‡æœ¬å†…å®¹ï¼š{content}

        è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        {{
            "entities": [
                {{
                    "name": "å®ä½“åç§°",
                    "type": "å®ä½“ç±»å‹ï¼ˆperson/organization/concept/technologyï¼‰",
                    "description": "å®ä½“æè¿°"
                }}
            ]
        }}
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        
        try:
            result = chain.invoke({"content": content[:1500]})
            entities = []
            
            for entity_data in result.get("entities", []):
                entity_id = hashlib.md5(f"{entity_data['name']}_{entity_data['type']}".encode()).hexdigest()
                
                entity = KnowledgeEntity(
                    id=entity_id,
                    name=entity_data["name"],
                    entity_type=entity_data["type"],
                    description=entity_data["description"],
                    confidence=0.8,  # ç®€åŒ–çš„ç½®ä¿¡åº¦
                    source_documents=[doc_id]
                )
                entities.append(entity)
            
            return entities
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            return []
    
    def process_document(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
            
            # åŠ è½½æ–‡æ¡£å†…å®¹
            if file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_path.endswith('.csv'):
                loader = CSVLoader(file_path, encoding='utf-8')
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                return False
            
            documents = loader.load()
            if not documents:
                logger.warning(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path}")
                return False
            
            content = documents[0].page_content
            
            # æå–å…ƒæ•°æ®
            metadata = self.extract_metadata(file_path, content)
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
            existing_doc = self.db_manager.get_document(metadata.id)
            if existing_doc and existing_doc.content_hash == metadata.content_hash:
                logger.info(f"æ–‡æ¡£æœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return True
            
            # ç”Ÿæˆæ‘˜è¦
            metadata.summary = self.generate_summary(content)
            
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents(documents)
            
            # æ›´æ–°å‘é‡å­˜å‚¨
            if self.vector_store and chunks:
                # ä¸ºæ¯ä¸ªchunkæ·»åŠ æ–‡æ¡£ID
                for chunk in chunks:
                    chunk.metadata['doc_id'] = metadata.id
                    chunk.metadata['doc_title'] = metadata.title
                
                self.vector_store.add_documents(chunks)
                self.save_vector_store()
            
            # æå–çŸ¥è¯†å®ä½“
            entities = self.extract_entities(content, metadata.id)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.db_manager.save_document(metadata)
            for entity in entities:
                self.db_manager.save_entity(entity)
            
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path}")
            logger.info(f"  - ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æ¡£å—")
            logger.info(f"  - æå–äº† {len(entities)} ä¸ªçŸ¥è¯†å®ä½“")
            
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            return False
    
    def process_directory(self, directory_path: str) -> Dict[str, Any]:
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        logger.info(f"å¼€å§‹å¤„ç†ç›®å½•: {directory_path}")
        
        results = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "skipped_files": 0
        }
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = ['.txt', '.csv', '.md']
        
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower()
                
                results["total_files"] += 1
                
                if file_ext not in supported_extensions:
                    results["skipped_files"] += 1
                    continue
                
                if self.process_document(file_path):
                    results["processed_files"] += 1
                else:
                    results["failed_files"] += 1
        
        logger.info(f"ç›®å½•å¤„ç†å®Œæˆ: {results}")
        return results

# =========================== æ™ºèƒ½æ£€ç´¢å™¨ ===========================

class IntelligentRetriever:
    """æ™ºèƒ½æ£€ç´¢å™¨"""
    
    def __init__(self, document_processor: DocumentProcessor):
        self.doc_processor = document_processor
        self.llm = document_processor.llm
        
    def semantic_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æ£€ç´¢"""
        if not self.doc_processor.vector_store:
            return []
        
        try:
            docs = self.doc_processor.vector_store.similarity_search_with_score(query, k=k)
            results = []
            
            for doc, score in docs:
                results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": score
                })
            
            return results
        except Exception as e:
            logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def query_expansion(self, query: str) -> List[str]:
        """æŸ¥è¯¢æ‰©å±•"""
        prompt = ChatPromptTemplate.from_template("""
        ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆ3-5ä¸ªç›¸å…³çš„æ‰©å±•æŸ¥è¯¢ï¼Œä»¥æé«˜æ£€ç´¢æ•ˆæœï¼š

        åŸå§‹æŸ¥è¯¢ï¼š{query}

        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{"expanded_queries": ["æ‰©å±•æŸ¥è¯¢1", "æ‰©å±•æŸ¥è¯¢2", "æ‰©å±•æŸ¥è¯¢3"]}}
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        
        try:
            result = chain.invoke({"query": query})
            return result.get("expanded_queries", [query])
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [query]
    
    def multi_query_retrieval(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        """å¤šæŸ¥è¯¢æ£€ç´¢"""
        expanded_queries = self.query_expansion(query)
        all_results = []
        seen_contents = set()
        
        for expanded_query in expanded_queries:
            results = self.semantic_search(expanded_query, k=k)
            
            for result in results:
                content = result["content"]
                if content not in seen_contents:
                    seen_contents.add(content)
                    all_results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        all_results.sort(key=lambda x: x["score"])
        return all_results[:k*2]  # è¿”å›æ›´å¤šç»“æœ

# =========================== æ™ºèƒ½å·¥å…· ===========================

class KnowledgeSystemTools:
    """çŸ¥è¯†ç³»ç»Ÿå·¥å…·é›†"""
    
    def __init__(self, db_manager: DatabaseManager, retriever: IntelligentRetriever):
        self.db_manager = db_manager
        self.retriever = retriever
    
    @tool
    def search_documents(self, query: str, limit: int = 5) -> str:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        # è¿™é‡Œéœ€è¦è®¿é—®å®ä¾‹å˜é‡ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦è°ƒæ•´
        return f"æœç´¢ç»“æœï¼šæ‰¾åˆ°{limit}ä¸ªç›¸å…³æ–‡æ¡£å…³äº'{query}'"
    
    @tool 
    def get_document_summary(self, doc_id: str) -> str:
        """è·å–æ–‡æ¡£æ‘˜è¦"""
        return f"æ–‡æ¡£{doc_id}çš„æ‘˜è¦"
    
    @tool
    def extract_key_concepts(self, content: str) -> str:
        """æå–å…³é”®æ¦‚å¿µ"""
        return f"ä»å†…å®¹ä¸­æå–çš„å…³é”®æ¦‚å¿µ"

# =========================== æ™ºèƒ½é—®ç­”ç³»ç»Ÿ ===========================

class IntelligentQASystem:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, db_manager: DatabaseManager, retriever: IntelligentRetriever):
        self.db_manager = db_manager
        self.retriever = retriever
        
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
        
        # åˆ›å»ºRAGé“¾
        self.create_rag_chain()
    
    def create_rag_chain(self):
        """åˆ›å»ºRAGå¤„ç†é“¾"""
        self.rag_prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†ç®¡ç†åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        ç›¸å…³æ–‡æ¡£ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
        åŒæ—¶ï¼Œè¯·æŒ‡å‡ºç­”æ¡ˆçš„æ¥æºæ–‡æ¡£ã€‚

        å›ç­”ï¼š
        """)
        
        self.rag_chain = (
            RunnableParallel({
                "context": RunnableLambda(lambda x: self.get_context(x["question"])),
                "question": RunnablePassthrough()
            })
            | self.rag_prompt
            | self.llm 
            | StrOutputParser()
        )
    
    def get_context(self, question: str) -> str:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢
        results = self.retriever.multi_query_retrieval(question, k=4)
        
        context_parts = []
        for i, result in enumerate(results, 1):
            doc_title = result["metadata"].get("doc_title", "æœªçŸ¥æ–‡æ¡£")
            content = result["content"][:500]  # é™åˆ¶é•¿åº¦
            context_parts.append(f"æ–‡æ¡£{i}ï¼ˆ{doc_title}ï¼‰ï¼š\\n{content}\\n")
        
        return "\\n".join(context_parts)
    
    def answer_question(self, question: str, session_id: Optional[str] = None) -> QueryResult:
        """å›ç­”é—®é¢˜"""
        start_time = time.time()
        
        try:
            # ç”Ÿæˆç­”æ¡ˆ
            answer = self.rag_chain.invoke({"question": question})
            
            # è·å–ç›¸å…³æ–‡æ¡£
            results = self.retriever.multi_query_retrieval(question, k=3)
            sources = [result["metadata"].get("doc_title", "æœªçŸ¥æ–‡æ¡£") for result in results]
            
            processing_time = time.time() - start_time
            
            return QueryResult(
                query=question,
                answer=answer,
                confidence=0.8,  # ç®€åŒ–çš„ç½®ä¿¡åº¦
                sources=sources,
                related_entities=[],  # å¯ä»¥è¿›ä¸€æ­¥å®ç°
                suggestions=[],      # å¯ä»¥è¿›ä¸€æ­¥å®ç°
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"é—®ç­”å¤„ç†å¤±è´¥: {e}")
            return QueryResult(
                query=question,
                answer=f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                confidence=0.0,
                sources=[],
                related_entities=[],
                suggestions=[],
                processing_time=time.time() - start_time
            )

# =========================== ä¸»ç³»ç»Ÿ ===========================

class IntelligentKnowledgeSystem:
    """æ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ"""
    
    def __init__(self, data_directory: str = "data"):
        self.data_directory = data_directory
        os.makedirs(data_directory, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db_manager = DatabaseManager()
        self.doc_processor = DocumentProcessor(self.db_manager)
        self.retriever = IntelligentRetriever(self.doc_processor)
        self.qa_system = IntelligentQASystem(self.db_manager, self.retriever)
        
        logger.info("æ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def add_documents_from_directory(self, directory_path: str) -> Dict[str, Any]:
        """ä»ç›®å½•æ·»åŠ æ–‡æ¡£"""
        return self.doc_processor.process_directory(directory_path)
    
    def add_document_from_text(self, title: str, content: str, author: str = "") -> bool:
        """ä»æ–‡æœ¬æ·»åŠ æ–‡æ¡£"""
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = os.path.join(self.data_directory, f"temp_{title}.txt")
        
        try:
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            success = self.doc_processor.process_document(temp_file)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
            
            return success
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æœ¬æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def ask_question(self, question: str, session_id: Optional[str] = None) -> QueryResult:
        """è¯¢é—®é—®é¢˜"""
        return self.qa_system.answer_question(question, session_id)
    
    def search_documents(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£"""
        return self.retriever.multi_query_retrieval(query, k=limit)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´è¯¦ç»†çš„ç»Ÿè®¡
        return {
            "total_documents": "N/A",  # éœ€è¦æŸ¥è¯¢æ•°æ®åº“
            "total_entities": "N/A",   # éœ€è¦æŸ¥è¯¢æ•°æ®åº“
            "vector_store_size": "N/A", # éœ€è¦æ£€æŸ¥å‘é‡å­˜å‚¨
            "system_status": "running"
        }

# =========================== æ¼”ç¤ºåŠŸèƒ½ ===========================

def create_sample_documents(system: IntelligentKnowledgeSystem):
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    print("ğŸ“š åˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
    
    sample_docs = [
        {
            "title": "äººå·¥æ™ºèƒ½å‘å±•å²",
            "content": """
            äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰çš„å‘å±•å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚

            ## æ—©æœŸå‘å±•ï¼ˆ1950s-1960sï¼‰
            1950å¹´ï¼Œè‹±å›½æ•°å­¦å®¶è‰¾ä¼¦Â·å›¾çµæå‡ºäº†è‘—åçš„å›¾çµæµ‹è¯•ï¼Œä¸ºäººå·¥æ™ºèƒ½çš„è¯„ä¼°æ ‡å‡†å¥ å®šäº†åŸºç¡€ã€‚
            1956å¹´ï¼Œçº¦ç¿°Â·éº¦å¡é”¡åœ¨è¾¾ç‰¹èŒ…æ–¯å­¦é™¢ç»„ç»‡äº†äººå·¥æ™ºèƒ½ç ”è®¨ä¼šï¼Œæ­£å¼æå‡ºäº†"äººå·¥æ™ºèƒ½"è¿™ä¸€æœ¯è¯­ã€‚

            ## ç¬¦å·ä¸»ä¹‰æ—¶æœŸï¼ˆ1960s-1980sï¼‰
            è¿™ä¸€æ—¶æœŸçš„AIä¸»è¦åŸºäºç¬¦å·é€»è¾‘å’ŒçŸ¥è¯†è¡¨ç¤ºã€‚ä»£è¡¨æ€§æˆæœåŒ…æ‹¬ä¸“å®¶ç³»ç»Ÿçš„å‘å±•ã€‚

            ## è¿æ¥ä¸»ä¹‰å¤å…´ï¼ˆ1980s-1990sï¼‰
            ç¥ç»ç½‘ç»œæŠ€æœ¯é‡æ–°å…´èµ·ï¼Œåå‘ä¼ æ’­ç®—æ³•çš„æå‡ºæ¨åŠ¨äº†æœºå™¨å­¦ä¹ çš„å‘å±•ã€‚

            ## ç°ä»£AIæ—¶ä»£ï¼ˆ2000s-è‡³ä»Šï¼‰
            æ·±åº¦å­¦ä¹ ã€å¤§æ•°æ®å’Œè®¡ç®—èƒ½åŠ›çš„æå‡ï¼Œä½¿AIåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—çªç ´æ€§è¿›å±•ã€‚
            """,
            "author": "AIå†å²å­¦å®¶"
        },
        {
            "title": "æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ",
            "content": """
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚

            ## ä¸»è¦ç±»å‹
            
            ### ç›‘ç£å­¦ä¹ 
            - ä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒ
            - åŒ…æ‹¬åˆ†ç±»å’Œå›å½’é—®é¢˜
            - å¸¸è§ç®—æ³•ï¼šçº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœº
            
            ### æ— ç›‘ç£å­¦ä¹ 
            - ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼
            - åŒ…æ‹¬èšç±»ã€é™ç»´ã€å…³è”è§„åˆ™å­¦ä¹ 
            - å¸¸è§ç®—æ³•ï¼šK-meansã€ä¸»æˆåˆ†åˆ†æã€å±‚æ¬¡èšç±»
            
            ### å¼ºåŒ–å­¦ä¹ 
            - é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥
            - åº”ç”¨äºæ¸¸æˆã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿ
            - ä»£è¡¨ç®—æ³•ï¼šQ-learningã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•

            ## å…³é”®æ¦‚å¿µ
            - ç‰¹å¾å·¥ç¨‹ï¼šä»åŸå§‹æ•°æ®ä¸­æå–æœ‰ç”¨ç‰¹å¾
            - æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨äº¤å‰éªŒè¯ç­‰æ–¹æ³•è¯„ä¼°æ¨¡å‹æ€§èƒ½
            - è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆï¼šæ¨¡å‹å¤æ‚åº¦çš„å¹³è¡¡
            """,
            "author": "æœºå™¨å­¦ä¹ ä¸“å®¶"
        },
        {
            "title": "æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œ",
            "content": """
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ å’Œå†³ç­–ã€‚

            ## ç¥ç»ç½‘ç»œåŸºç¡€
            äººå·¥ç¥ç»ç½‘ç»œæ¨¡ä»¿äººè„‘ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ï¼Œç”±è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚

            ### æ ¸å¿ƒç»„ä»¶
            - ç¥ç»å…ƒï¼šåŸºæœ¬è®¡ç®—å•å…ƒ
            - æƒé‡å’Œåç½®ï¼šæ§åˆ¶ä¿¡å·ä¼ é€’çš„å‚æ•°
            - æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§ç‰¹æ€§

            ## æ·±åº¦å­¦ä¹ æ¶æ„

            ### å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
            - ä¸»è¦ç”¨äºå›¾åƒå¤„ç†
            - åŒ…å«å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚
            - åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°ä¼˜å¼‚

            ### å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
            - é€‚ç”¨äºåºåˆ—æ•°æ®å¤„ç†
            - LSTMå’ŒGRUè§£å†³äº†é•¿æœŸä¾èµ–é—®é¢˜
            - å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†

            ### Transformeræ¶æ„
            - åŸºäºæ³¨æ„åŠ›æœºåˆ¶
            - GPTã€BERTç­‰æ¨¡å‹çš„åŸºç¡€
            - åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—çªç ´

            ## åº”ç”¨é¢†åŸŸ
            - è®¡ç®—æœºè§†è§‰ï¼šå›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒç”Ÿæˆ
            - è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æ
            - è¯­éŸ³æŠ€æœ¯ï¼šè¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆ
            """,
            "author": "æ·±åº¦å­¦ä¹ ç ”ç©¶å‘˜"
        }
    ]
    
    success_count = 0
    for doc in sample_docs:
        if system.add_document_from_text(doc["title"], doc["content"], doc["author"]):
            success_count += 1
            print(f"âœ… æˆåŠŸæ·»åŠ æ–‡æ¡£: {doc['title']}")
        else:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {doc['title']}")
    
    print(f"ğŸ“Š æˆåŠŸæ·»åŠ  {success_count}/{len(sample_docs)} ä¸ªæ–‡æ¡£")

def demo_question_answering(system: IntelligentKnowledgeSystem):
    """æ¼”ç¤ºé—®ç­”åŠŸèƒ½"""
    print("\\nğŸ’¬ æ™ºèƒ½é—®ç­”æ¼”ç¤º")
    print("=" * 50)
    
    test_questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦ç±»å‹ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "Transformeræ¶æ„çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "è°æå‡ºäº†å›¾çµæµ‹è¯•ï¼Ÿ",
        "CNNä¸»è¦ç”¨äºä»€ä¹ˆé¢†åŸŸï¼Ÿ"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\\nâ“ é—®é¢˜ {i}: {question}")
        print("-" * 40)
        
        result = system.ask_question(question)
        
        print(f"ğŸ¤– å›ç­”: {result.answer}")
        print(f"ğŸ“š æ¥æºæ–‡æ¡£: {', '.join(result.sources)}")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {result.confidence:.2f}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.3f}ç§’")

def demo_document_search(system: IntelligentKnowledgeSystem):
    """æ¼”ç¤ºæ–‡æ¡£æœç´¢"""
    print("\\nğŸ” æ–‡æ¡£æœç´¢æ¼”ç¤º")
    print("=" * 50)
    
    search_queries = [
        "ç¥ç»ç½‘ç»œ",
        "ç›‘ç£å­¦ä¹ ",
        "å›¾åƒè¯†åˆ«",
        "è‡ªç„¶è¯­è¨€å¤„ç†"
    ]
    
    for query in search_queries:
        print(f"\\nğŸ” æœç´¢: {query}")
        print("-" * 30)
        
        results = system.search_documents(query, limit=3)
        
        for i, result in enumerate(results, 1):
            doc_title = result["metadata"].get("doc_title", "æœªçŸ¥æ–‡æ¡£")
            content_preview = result["content"][:150] + "..."
            score = result["score"]
            
            print(f"  {i}. {doc_title} (ç›¸ä¼¼åº¦: {score:.4f})")
            print(f"     {content_preview}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ§  LangChain Challenge 8: æ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("ğŸš€ åˆå§‹åŒ–æ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ...")
        system = IntelligentKnowledgeSystem()
        
        # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
        create_sample_documents(system)
        
        # æ¼”ç¤ºé—®ç­”åŠŸèƒ½
        demo_question_answering(system)
        
        # æ¼”ç¤ºæ–‡æ¡£æœç´¢
        demo_document_search(system)
        
        # æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡
        print("\\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print("=" * 50)
        stats = system.get_system_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("\\n" + "=" * 60)
        print("ğŸ¯ ç³»ç»ŸåŠŸèƒ½ç‰¹ç‚¹:")
        print("âœ… å¤šæ ¼å¼æ–‡æ¡£å¤„ç†å’Œç´¢å¼•")
        print("âœ… æ™ºèƒ½æ–‡æ¡£æœç´¢å’Œæ£€ç´¢")
        print("âœ… åŸºäºRAGçš„é—®ç­”ç³»ç»Ÿ")
        print("âœ… çŸ¥è¯†å®ä½“æå–å’Œç®¡ç†")
        print("âœ… æ–‡æ¡£æ‘˜è¦è‡ªåŠ¨ç”Ÿæˆ")
        print("âœ… å‘é‡åŒ–å­˜å‚¨å’Œè¯­ä¹‰æ£€ç´¢")
        print("âœ… æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨")
        print("âœ… æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ¶æ„")
        
        print("\\nğŸš€ æ‰©å±•å»ºè®®:")
        print("1. æ·»åŠ Webç•Œé¢å’ŒAPIæœåŠ¡")
        print("2. å®ç°å¤šç”¨æˆ·æƒé™ç®¡ç†")
        print("3. é›†æˆæ›´å¤šæ–‡æ¡£æ ¼å¼æ”¯æŒ")
        print("4. æ·»åŠ çŸ¥è¯†å›¾è°±å¯è§†åŒ–")
        print("5. å®ç°åˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—")
        print("6. æ·»åŠ å®æ—¶åä½œåŠŸèƒ½")
        print("7. é›†æˆå¤–éƒ¨æ•°æ®æº")
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        print("\\nè¯·ç¡®ä¿:")
        print("1. å·²è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("2. å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…:")
        print("   pip install langchain langchain-openai langchain-community")
        print("   pip install faiss-cpu sqlite3")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import time
    main()
