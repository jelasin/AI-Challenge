#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Challenge 1: LangChain v0.3 é£æ ¼çš„åŸºç¡€ç¿»è¯‘å™¨ï¼ˆLCEL + Streaming + å¯é€‰ç»“æ„åŒ–è¾“å‡ºï¼‰

è¦ç‚¹ï¼ˆv0.3 æ¨èèŒƒå¼ï¼‰ï¼š
- ä½¿ç”¨ ChatPromptTemplate ç»„ç»‡æ¶ˆæ¯ï¼ˆsystem/humanï¼‰
- é‡‡ç”¨ LCELï¼šprompt | model | parser
- ä½¿ç”¨ chain.stream(input) åšæµå¼è¾“å‡º
- é€šè¿‡ with_structured_output(PydanticModel) è·å¾—ç»“æ„åŒ–ç»“æœï¼ˆå¯é€‰ï¼‰
"""

from __future__ import annotations

import os
import sys
import argparse
from typing import Optional, cast

from pydantic import BaseModel, Field

from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


class TranslationResult(BaseModel):
    """ç¿»è¯‘ç»“æœç»“æ„ï¼ˆç”¨äºå¯é€‰çš„ç»“æ„åŒ–è¾“å‡ºæ¨¡å¼ï¼‰ã€‚"""

    translated_text: str = Field(description="ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä»…åŒ…å«è¯‘æ–‡æœ¬èº«")
    source_language: str = Field(description="æ£€æµ‹åˆ°çš„æºè¯­è¨€æˆ–ç”¨æˆ·å£°æ˜çš„æºè¯­è¨€")
    target_language: str = Field(description="ç›®æ ‡è¯­è¨€")
    confidence: Optional[str] = Field(
        description="è¯‘æ–‡è´¨é‡ç½®ä¿¡åº¦ï¼ˆæ¨¡å‹ä¸»è§‚è¯„ä¼°ï¼‰", default="high"
    )


def build_stream_chain(model):
    """æ„å»ºæµå¼è¾“å‡ºçš„ LCEL ç¿»è¯‘é“¾ï¼šprompt | model | StrOutputParserã€‚"""

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚åªè¾“å‡ºæœ€ç»ˆè¯‘æ–‡ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦é™„åŠ è¯´æ˜ã€‚"
                "ä¿æŒè¯­ä¹‰å’Œè¯­æ°”è‡ªç„¶å‡†ç¡®ï¼Œä¸“æœ‰åè¯é‡‡ç”¨å¸¸è§/æ ‡å‡†è¯‘æ³•ã€‚",
            ),
            (
                "human",
                "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{src_language}ç¿»è¯‘æˆ{dst_language}ï¼š\n\n{text_message}",
            ),
        ]
    )
    return prompt | model | StrOutputParser()


def build_structured_chain(model):
    """æ„å»ºç»“æ„åŒ–è¾“å‡ºé“¾ï¼šprompt | model.with_structured_output(TranslationResult)ã€‚"""

    structured_model = model.with_structured_output(TranslationResult)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¿”å›çš„å†…å®¹å¿…é¡»ä¸¥æ ¼åŒ¹é…ç»™å®šçš„æ¨¡å¼ã€‚"
                "è¦æ±‚ï¼š\n"
                "- translated_text ä»…åŒ…å«è¯‘æ–‡ï¼Œä¸å¾—åŒ…å«ä»»ä½•è§£é‡Šï¼›\n"
                "- confidence å¿…é¡»æ˜¯ 'low'ã€'medium' æˆ– 'high' ä¹‹ä¸€ï¼›æ— æ³•åˆ¤æ–­æ—¶ç”¨ 'high'ï¼›ä¸å¾—è¿”å› null/Noneã€‚",
            ),
            (
                "human",
                "ä»{src_language}ç¿»è¯‘åˆ°{dst_language}ï¼š\n\n{text_message}\n\n"
                "è¯·æŒ‰æ¨¡å¼è¿”å›å­—æ®µï¼ˆtranslated_text/source_language/target_language/confidenceï¼‰ï¼Œ"
                "å…¶ä¸­ confidence âˆˆ {{low, medium, high}}ã€‚",
            ),
        ]
    )
    return prompt | structured_model


def ensure_api_key() -> bool:
    """æ£€æŸ¥ OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚"""

    if os.getenv("OPENAI_API_KEY"):
        return True
    print("âŒ æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    print("ğŸ’¡ åœ¨ PowerShell ä¸‹å¯æ‰§è¡Œï¼š")
    print("   $env:OPENAI_API_KEY = 'your-openai-key'")
    print("æˆ–åœ¨ Linux/macOSï¼š")
    print("   export OPENAI_API_KEY='your-openai-key'")
    return False


def parse_args(argv: list[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="LangChain v0.3 é£æ ¼ç¿»è¯‘å™¨ï¼ˆæ”¯æŒæµå¼ä¸ç»“æ„åŒ–ä¸¤ç§æ¨¡å¼ï¼‰"
    )
    parser.add_argument("--src", dest="src_language", default="è‡ªåŠ¨æ£€æµ‹", help="æºè¯­è¨€")
    parser.add_argument("--dst", dest="dst_language", required=False, help="ç›®æ ‡è¯­è¨€")
    parser.add_argument("--text", dest="text_message", required=False, help="å¾…ç¿»è¯‘æ–‡æœ¬")
    parser.add_argument(
        "--mode",
        choices=["stream", "structured"],
        default="stream",
        help="è¾“å‡ºæ¨¡å¼ï¼šstream=æµå¼å­—ç¬¦è¾“å‡ºï¼Œstructured=è¿”å›ç»“æ„åŒ–å¯¹è±¡",
    )
    parser.add_argument(
        "--model",
        default="gpt-4o-mini",
        help="OpenAI æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šgpt-4o-miniï¼‰",
    )
    return parser.parse_args(argv)


def interactive_fill(args: argparse.Namespace) -> argparse.Namespace:
    """äº¤äº’å¼è¡¥å…¨ç¼ºå¤±å‚æ•°ã€‚"""

    print("\nğŸŒ æ¬¢è¿ä½¿ç”¨ LangChain v0.3 æ™ºèƒ½ç¿»è¯‘å™¨")
    print("-" * 48)
    if not args.src_language:
        args.src_language = input("ğŸ“ è¯·è¾“å…¥æºè¯­è¨€ (å¦‚: ä¸­æ–‡, Englishï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹): ").strip() or "è‡ªåŠ¨æ£€æµ‹"
    if not args.dst_language:
        args.dst_language = input("ğŸ¯ è¯·è¾“å…¥ç›®æ ‡è¯­è¨€ (å¦‚: è‹±æ–‡, ä¸­æ–‡): ").strip()
    if not args.text_message:
        args.text_message = input("ğŸ“„ è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬: ").strip()
    return args


def run_stream_mode(model_name: str, src_language: str, dst_language: str, text_message: str) -> None:
    try:
        # ä½¿ç”¨ LangChain æä¾›çš„é€šç”¨åˆå§‹åŒ–æ–¹æ³•ï¼Œè‡ªåŠ¨æ¨æ–­æä¾›å•†
        llm = init_chat_model(model_name, temperature=0)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–æ¨¡å‹å¤±è´¥ï¼š{e}")
        return

    chain = build_stream_chain(llm)
    inputs = {
        "src_language": src_language or "è‡ªåŠ¨æ£€æµ‹",
        "dst_language": dst_language,
        "text_message": text_message,
    }

    print("\nğŸ“¡ æ­£åœ¨æµå¼ç”Ÿæˆè¯‘æ–‡ï¼š\n")
    collected = []
    try:
        for chunk in chain.stream(inputs):  # å¢é‡ token è¾“å‡º
            print(chunk, end="", flush=True)
            collected.append(chunk)
        translated = ("".join(collected)).strip()

        print("\n" + "=" * 60)
        print("ğŸ‰ ç¿»è¯‘å®Œæˆï¼ˆæµå¼ï¼‰")
        print("=" * 60)
        print(f"ğŸ”¤ æºè¯­è¨€ï¼š{src_language or 'è‡ªåŠ¨æ£€æµ‹'}")
        print(f"ğŸ¯ ç›®æ ‡è¯­è¨€ï¼š{dst_language}")
        print(f"ğŸ“„ åŸæ–‡ï¼š{text_message}")
        print(f"âœ¨ è¯‘æ–‡ï¼š{translated}")
        print("=" * 60)
    except Exception as e:
        print(f"\nâŒ ç¿»è¯‘å¤±è´¥ï¼š{e}")


def run_structured_mode(model_name: str, src_language: str, dst_language: str, text_message: str) -> None:
    try:
        # ä½¿ç”¨ LangChain æä¾›çš„é€šç”¨åˆå§‹åŒ–æ–¹æ³•ï¼Œè‡ªåŠ¨æ¨æ–­æä¾›å•†
        llm = init_chat_model(model_name, temperature=0)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–æ¨¡å‹å¤±è´¥ï¼š{e}")
        return

    chain = build_structured_chain(llm)
    inputs = {
        "src_language": src_language or "è‡ªåŠ¨æ£€æµ‹",
        "dst_language": dst_language,
        "text_message": text_message,
    }

    print("\nğŸ§© æ­£åœ¨ç”Ÿæˆç»“æ„åŒ–ç»“æœ...\n")
    try:
        result = cast(TranslationResult, chain.invoke(inputs))
        print("=" * 60)
        print("ğŸ‰ ç¿»è¯‘å®Œæˆï¼ˆç»“æ„åŒ–ï¼‰")
        print("=" * 60)
        print(f"ğŸ”¤ æºè¯­è¨€ï¼š{result.source_language}")
        print(f"ğŸ¯ ç›®æ ‡è¯­è¨€ï¼š{result.target_language}")
        print(f"âœ¨ è¯‘æ–‡ï¼š{result.translated_text}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦ï¼š{result.confidence}")
        print("=" * 60)
    except Exception as e:
        print(f"âŒ ç»“æ„åŒ–æ¨¡å¼å¤±è´¥ï¼š{e}")


def main(argv: list[str]) -> int:
    if not ensure_api_key():
        return 1

    args = parse_args(argv)
    args = interactive_fill(args)

    # æ ¡éªŒå¿…è¦å­—æ®µ
    if not args.dst_language:
        print("âŒ ç›®æ ‡è¯­è¨€ä¸èƒ½ä¸ºç©ºã€‚")
        return 2
    if not args.text_message:
        print("âŒ å¾…ç¿»è¯‘æ–‡æœ¬ä¸èƒ½ä¸ºç©ºã€‚")
        return 2

    if args.mode == "structured":
        run_structured_mode(args.model, args.src_language, args.dst_language, args.text_message)
    else:
        run_stream_mode(args.model, args.src_language, args.dst_language, args.text_message)

    return 0


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
