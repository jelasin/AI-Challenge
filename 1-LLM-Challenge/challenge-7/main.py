# -*- coding: utf-8 -*-
"""
Challenge 7: æµå¼å¤„ç†å’Œå¼‚æ­¥æ“ä½œ
éš¾åº¦ï¼šé«˜çº§

å­¦ä¹ ç›®æ ‡ï¼š
1. æŒæ¡LangChainçš„æµå¼è¾“å‡º
2. å­¦ä¹ å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼
3. å®ç°äº‹ä»¶æµå¤„ç†
4. æ„å»ºå®æ—¶èŠå¤©ç³»ç»Ÿ
5. ä¼˜åŒ–å¤§è§„æ¨¡å¹¶å‘å¤„ç†
6. å®ç°æµå¼æ•°æ®ç®¡é“

ä»»åŠ¡æè¿°ï¼š
åˆ›å»ºä¸€ä¸ªé«˜æ€§èƒ½çš„å®æ—¶AIåº”ç”¨ï¼Œèƒ½å¤Ÿï¼š
1. æ”¯æŒæµå¼å¯¹è¯å’Œå®æ—¶å“åº”
2. å¤„ç†å¤§é‡å¹¶å‘è¯·æ±‚
3. å®ç°äº‹ä»¶é©±åŠ¨çš„å¤„ç†æµç¨‹
4. æ„å»ºæµå¼æ•°æ®å¤„ç†ç®¡é“
5. ç›‘æ§å’Œä¼˜åŒ–æ€§èƒ½
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.callbacks import AsyncCallbackHandler, BaseCallbackHandler
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from typing import List, Dict, Any, Optional, AsyncIterator, Iterator, Callable
import asyncio
import aiohttp
import time
import json
import threading
from datetime import datetime
from dataclasses import dataclass
from collections import deque
import logging
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class StreamEvent:
    """æµäº‹ä»¶æ•°æ®ç±»"""
    event_type: str
    timestamp: datetime
    data: Any
    session_id: Optional[str] = None

class StreamingCallbackHandler(BaseCallbackHandler):
    """æµå¼å›è°ƒå¤„ç†å™¨"""
    
    def __init__(self, session_id: Optional[str] = None):
        self.session_id = session_id
        self.events = deque(maxlen=1000)  # ä¿æŒæœ€è¿‘1000ä¸ªäº‹ä»¶
        self.current_tokens = []
        
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLMå¼€å§‹æ—¶çš„å›è°ƒ"""
        event = StreamEvent("llm_start", datetime.now(), {"prompts": prompts}, self.session_id)
        self.events.append(event)
        print(f"ğŸš€ [{self.session_id}] LLMå¼€å§‹å¤„ç†...")
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """æ–°tokenç”Ÿæˆæ—¶çš„å›è°ƒ"""
        self.current_tokens.append(token)
        event = StreamEvent("new_token", datetime.now(), {"token": token}, self.session_id)
        self.events.append(event)
        print(f"ğŸ“ [{self.session_id}] æ–°token: '{token}'", end='', flush=True)
    
    def on_llm_end(self, response, **kwargs) -> None:
        """LLMç»“æŸæ—¶çš„å›è°ƒ"""
        full_text = ''.join(self.current_tokens)
        event = StreamEvent("llm_end", datetime.now(), {"response": full_text}, self.session_id)
        self.events.append(event)
        print(f"\\nâœ… [{self.session_id}] LLMå¤„ç†å®Œæˆ")
        self.current_tokens.clear()
    
    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """LLMé”™è¯¯æ—¶çš„å›è°ƒ"""
        event = StreamEvent("llm_error", datetime.now(), {"error": str(error)}, self.session_id)
        self.events.append(event)
        print(f"âŒ [{self.session_id}] LLMé”™è¯¯: {error}")

class AsyncStreamingCallbackHandler(AsyncCallbackHandler):
    """å¼‚æ­¥æµå¼å›è°ƒå¤„ç†å™¨"""
    
    def __init__(self, session_id: Optional[str] = None):
        self.session_id = session_id
        self.events = deque(maxlen=1000)
        self.current_tokens = []
        
    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """å¼‚æ­¥LLMå¼€å§‹å›è°ƒ"""
        event = StreamEvent("async_llm_start", datetime.now(), {"prompts": prompts}, self.session_id)
        self.events.append(event)
        print(f"ğŸš€ [ASYNC-{self.session_id}] LLMå¼€å§‹å¤„ç†...")
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """å¼‚æ­¥æ–°tokenå›è°ƒ"""
        self.current_tokens.append(token)
        event = StreamEvent("async_new_token", datetime.now(), {"token": token}, self.session_id)
        self.events.append(event)
        print(f"ğŸ“ [ASYNC-{self.session_id}] æ–°token: '{token}'", end='', flush=True)
    
    async def on_llm_end(self, response, **kwargs) -> None:
        """å¼‚æ­¥LLMç»“æŸå›è°ƒ"""
        full_text = ''.join(self.current_tokens)
        event = StreamEvent("async_llm_end", datetime.now(), {"response": full_text}, self.session_id)
        self.events.append(event)
        print(f"\\nâœ… [ASYNC-{self.session_id}] LLMå¤„ç†å®Œæˆ")
        self.current_tokens.clear()

class RealTimeChatSystem:
    """å®æ—¶èŠå¤©ç³»ç»Ÿ"""
    
    def __init__(self):
        self.sessions: Dict[str, Dict] = {}
        self.active_streams: Dict[str, bool] = {}
        
        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        # åˆ›å»ºLLMå®ä¾‹
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.7,
            streaming=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # åˆ›å»ºåŸºç¡€é“¾
        prompt = ChatPromptTemplate.from_template(
            "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨è½»æ¾å¯¹è¯çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š{question}"
        )
        
        self.base_chain = prompt | self.llm | StrOutputParser()
    
    def create_session(self, session_id: str) -> None:
        """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
        self.sessions[session_id] = {
            "history": [],
            "created_at": datetime.now(),
            "callback_handler": StreamingCallbackHandler(session_id)
        }
        self.active_streams[session_id] = False
        print(f"ğŸ“± åˆ›å»ºæ–°ä¼šè¯: {session_id}")
    
    def stream_response(self, session_id: str, question: str) -> Iterator[str]:
        """æµå¼å“åº”ç”Ÿæˆå™¨"""
        if session_id not in self.sessions:
            self.create_session(session_id)
        
        session = self.sessions[session_id]
        callback_handler = session["callback_handler"]
        
        self.active_streams[session_id] = True
        
        try:
            # ä½¿ç”¨æµå¼å¤„ç†
            for chunk in self.base_chain.stream(
                {"question": question}, 
                config={"callbacks": [callback_handler]}
            ):
                if self.active_streams[session_id]:  # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­æµå¼ä¼ è¾“
                    yield chunk
                else:
                    break
            
            # æ›´æ–°ä¼šè¯å†å²
            session["history"].append({"user": question, "assistant": "".join(chunk for chunk in self.base_chain.stream({"question": question}))})
            
        except Exception as e:
            yield f"é”™è¯¯: {str(e)}"
        finally:
            self.active_streams[session_id] = False
    
    def stop_stream(self, session_id: str) -> None:
        """åœæ­¢æŒ‡å®šä¼šè¯çš„æµå¼ä¼ è¾“"""
        if session_id in self.active_streams:
            self.active_streams[session_id] = False
            print(f"â¹ï¸ åœæ­¢ä¼šè¯ {session_id} çš„æµå¼ä¼ è¾“")

class AsyncProcessingPipeline:
    """å¼‚æ­¥å¤„ç†ç®¡é“"""
    
    def __init__(self):
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.1,
            streaming=True
        )
        
        self.processing_queue = asyncio.Queue()
        self.results_cache: Dict[str, Any] = {}
        
    async def add_task(self, task_id: str, content: str, task_type: str = "general"):
        """æ·»åŠ ä»»åŠ¡åˆ°å¤„ç†é˜Ÿåˆ—"""
        task = {
            "id": task_id,
            "content": content,
            "type": task_type,
            "timestamp": datetime.now(),
            "status": "queued"
        }
        await self.processing_queue.put(task)
        print(f"ğŸ“¥ ä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—: {task_id}")
    
    async def process_single_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        try:
            task["status"] = "processing"
            print(f"ğŸ”„ å¼€å§‹å¤„ç†ä»»åŠ¡: {task['id']}")
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†é€»è¾‘
            if task["type"] == "summarize":
                prompt = ChatPromptTemplate.from_template(
                    "è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š\\n{content}"
                )
            elif task["type"] == "translate":
                prompt = ChatPromptTemplate.from_template(
                    "è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆè‹±æ–‡ï¼š\\n{content}"
                )
            elif task["type"] == "analyze":
                prompt = ChatPromptTemplate.from_template(
                    "è¯·åˆ†æä»¥ä¸‹å†…å®¹çš„ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ï¼š\\n{content}"
                )
            else:
                prompt = ChatPromptTemplate.from_template(
                    "è¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š\\n{content}"
                )
            
            # åˆ›å»ºå¼‚æ­¥å›è°ƒå¤„ç†å™¨
            callback_handler = AsyncStreamingCallbackHandler(task["id"])
            
            # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
            chain = prompt | self.llm | StrOutputParser()
            result = await chain.ainvoke(
                {"content": task["content"]}, 
                config={"callbacks": [callback_handler]}
            )
            
            task["status"] = "completed"
            task["result"] = result
            task["completed_at"] = datetime.now()
            
            # ç¼“å­˜ç»“æœ
            self.results_cache[task["id"]] = task
            
            print(f"âœ… ä»»åŠ¡å¤„ç†å®Œæˆ: {task['id']}")
            return task
            
        except Exception as e:
            task["status"] = "failed"
            task["error"] = str(e)
            task["failed_at"] = datetime.now()
            print(f"âŒ ä»»åŠ¡å¤„ç†å¤±è´¥: {task['id']} - {e}")
            return task
    
    async def batch_process(self, max_concurrent: int = 5):
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
        
        async def worker():
            """å·¥ä½œåç¨‹"""
            while True:
                try:
                    # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                    task = await self.processing_queue.get()
                    
                    # å¤„ç†ä»»åŠ¡
                    await self.process_single_task(task)
                    
                    # æ ‡è®°ä»»åŠ¡å®Œæˆ
                    self.processing_queue.task_done()
                    
                except Exception as e:
                    print(f"Worker error: {e}")
        
        # åˆ›å»ºå·¥ä½œåç¨‹
        workers = [asyncio.create_task(worker()) for _ in range(max_concurrent)]
        
        try:
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await self.processing_queue.join()
        finally:
            # å–æ¶ˆå·¥ä½œåç¨‹
            for worker_task in workers:
                worker_task.cancel()
    
    def get_result(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        return self.results_cache.get(task_id)

class StreamingDataPipeline:
    """æµå¼æ•°æ®å¤„ç†ç®¡é“"""
    
    def __init__(self):
        self.processors: List[Callable] = []
        self.output_handlers: List[Callable] = []
        
    def add_processor(self, processor: Callable):
        """æ·»åŠ å¤„ç†å™¨"""
        self.processors.append(processor)
        
    def add_output_handler(self, handler: Callable):
        """æ·»åŠ è¾“å‡ºå¤„ç†å™¨"""
        self.output_handlers.append(handler)
    
    async def process_stream(self, data_stream: AsyncIterator[Any]):
        """å¤„ç†æ•°æ®æµ"""
        async for data in data_stream:
            # ä¾æ¬¡é€šè¿‡æ‰€æœ‰å¤„ç†å™¨
            processed_data = data
            for processor in self.processors:
                if asyncio.iscoroutinefunction(processor):
                    processed_data = await processor(processed_data)
                else:
                    processed_data = processor(processed_data)
            
            # å‘é€åˆ°æ‰€æœ‰è¾“å‡ºå¤„ç†å™¨
            for handler in self.output_handlers:
                if asyncio.iscoroutinefunction(handler):
                    await handler(processed_data)
                else:
                    handler(processed_data)

def demo_basic_streaming():
    """æ¼”ç¤ºåŸºæœ¬æµå¼å¤„ç†"""
    print("ğŸŒŠ åŸºæœ¬æµå¼å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºæµå¼LLM
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.7,
        streaming=True
    )
    
    prompt = ChatPromptTemplate.from_template("è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ï¼š{topic}")
    chain = prompt | llm | StrOutputParser()
    
    print("æµå¼è¾“å‡ºç¤ºä¾‹ï¼ˆä»‹ç»äººå·¥æ™ºèƒ½ï¼‰:")
    print("-" * 30)
    
    # æµå¼å¤„ç†
    for chunk in chain.stream({"topic": "äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"}):
        print(chunk, end="", flush=True)
    
    print("\\n")

def demo_real_time_chat():
    """æ¼”ç¤ºå®æ—¶èŠå¤©ç³»ç»Ÿ"""
    print("ğŸ’¬ å®æ—¶èŠå¤©ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    try:
        chat_system = RealTimeChatSystem()
        
        # åˆ›å»ºæµ‹è¯•ä¼šè¯
        session_id = "demo_session_001"
        chat_system.create_session(session_id)
        
        # æµ‹è¯•é—®é¢˜
        questions = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "èƒ½ç»™æˆ‘è®²ä¸ªç¬‘è¯å—ï¼Ÿ"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\\nğŸ‘¤ ç”¨æˆ·é—®é¢˜ {i}: {question}")
            print("ğŸ¤– AIå›ç­”: ", end="")
            
            # æµå¼å“åº”
            for chunk in chat_system.stream_response(session_id, question):
                print(chunk, end="", flush=True)
            
            print("\\n")
            
            # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
            time.sleep(1)
            
    except Exception as e:
        print(f"èŠå¤©ç³»ç»Ÿé”™è¯¯: {e}")

async def demo_async_processing():
    """æ¼”ç¤ºå¼‚æ­¥å¤„ç†ç®¡é“"""
    print("âš¡ å¼‚æ­¥å¤„ç†ç®¡é“æ¼”ç¤º")
    print("=" * 50)
    
    try:
        pipeline = AsyncProcessingPipeline()
        
        # æ·»åŠ æµ‹è¯•ä»»åŠ¡
        tasks = [
            ("task_001", "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚", "summarize"),
            ("task_002", "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸã€‚", "translate"),
            ("task_003", "æ·±åº¦å­¦ä¹ ã€ç¥ç»ç½‘ç»œã€è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„å…³é”®æŠ€æœ¯ã€‚", "analyze"),
            ("task_004", "Pythonæ˜¯AIå¼€å‘ä¸­æœ€æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ä¹‹ä¸€ã€‚", "summarize"),
            ("task_005", "æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ å¯†ä¸å¯åˆ†ã€‚", "translate")
        ]
        
        print(f"ğŸ“‹ æ·»åŠ  {len(tasks)} ä¸ªä»»åŠ¡åˆ°å¤„ç†é˜Ÿåˆ—...")
        
        # æ·»åŠ ä»»åŠ¡
        for task_id, content, task_type in tasks:
            await pipeline.add_task(task_id, content, task_type)
        
        # å¼€å§‹æ‰¹é‡å¤„ç†
        print("ğŸ”„ å¼€å§‹å¼‚æ­¥æ‰¹é‡å¤„ç†...")
        start_time = time.time()
        
        await pipeline.batch_process(max_concurrent=3)
        
        end_time = time.time()
        print(f"âœ… æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼Œç”¨æ—¶: {end_time - start_time:.2f}ç§’")
        
        # æ˜¾ç¤ºç»“æœ
        print("\\nğŸ“Š å¤„ç†ç»“æœ:")
        for task_id, _, _ in tasks:
            result = pipeline.get_result(task_id)
            if result:
                print(f"   {task_id}: {result['status']}")
                if result['status'] == 'completed':
                    print(f"     ç»“æœ: {result['result'][:100]}...")
                elif result['status'] == 'failed':
                    print(f"     é”™è¯¯: {result['error']}")
        
    except Exception as e:
        print(f"å¼‚æ­¥å¤„ç†é”™è¯¯: {e}")

async def demo_streaming_data_pipeline():
    """æ¼”ç¤ºæµå¼æ•°æ®ç®¡é“"""
    print("ğŸ”„ æµå¼æ•°æ®ç®¡é“æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®ç®¡é“
    pipeline = StreamingDataPipeline()
    
    # æ·»åŠ å¤„ç†å™¨
    def text_cleaner(data):
        """æ–‡æœ¬æ¸…ç†å™¨"""
        if isinstance(data, str):
            return data.strip().replace("\\n", " ")
        return data
    
    def text_analyzer(data):
        """æ–‡æœ¬åˆ†æå™¨"""
        if isinstance(data, str):
            word_count = len(data.split())
            return {
                "original": data,
                "word_count": word_count,
                "length": len(data),
                "timestamp": datetime.now()
            }
        return data
    
    async def result_handler(data):
        """ç»“æœå¤„ç†å™¨"""
        print(f"ğŸ“Š å¤„ç†ç»“æœ: é•¿åº¦={data['length']}, è¯æ•°={data['word_count']}")
        print(f"   å†…å®¹: {data['original'][:50]}...")
    
    pipeline.add_processor(text_cleaner)
    pipeline.add_processor(text_analyzer) 
    pipeline.add_output_handler(result_handler)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®æµ
    async def test_data_stream():
        test_texts = [
            "   äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œ   \\n",
            "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
            "æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†AIçš„å‘å±•",
            "è‡ªç„¶è¯­è¨€å¤„ç†è®©æœºå™¨ç†è§£äººç±»è¯­è¨€",
            "è®¡ç®—æœºè§†è§‰è®©æœºå™¨çœ‹æ‡‚å›¾åƒ"
        ]
        
        for text in test_texts:
            yield text
            await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿæ•°æ®åˆ°è¾¾é—´éš”
    
    print("å¼€å§‹å¤„ç†æµå¼æ•°æ®...")
    await pipeline.process_stream(test_data_stream())

def demo_performance_monitoring():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
    print("ğŸ“ˆ æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    print("=" * 50)
    
    class PerformanceMonitor:
        def __init__(self):
            self.metrics = {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "avg_response_time": 0,
                "response_times": []
            }
        
        def record_request(self, success: bool, response_time: float):
            """è®°å½•è¯·æ±‚"""
            self.metrics["total_requests"] += 1
            if success:
                self.metrics["successful_requests"] += 1
            else:
                self.metrics["failed_requests"] += 1
            
            self.metrics["response_times"].append(response_time)
            self.metrics["avg_response_time"] = sum(self.metrics["response_times"]) / len(self.metrics["response_times"])
        
        def get_stats(self) -> Dict[str, Any]:
            """è·å–ç»Ÿè®¡ä¿¡æ¯"""
            return {
                **self.metrics,
                "success_rate": self.metrics["successful_requests"] / max(1, self.metrics["total_requests"]),
                "min_response_time": min(self.metrics["response_times"]) if self.metrics["response_times"] else 0,
                "max_response_time": max(self.metrics["response_times"]) if self.metrics["response_times"] else 0
            }
    
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor()
    
    # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚
    import random
    for i in range(20):
        success = random.random() > 0.1  # 90%æˆåŠŸç‡
        response_time = random.uniform(0.1, 2.0)  # 0.1-2.0ç§’å“åº”æ—¶é—´
        monitor.record_request(success, response_time)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = monitor.get_stats()
    print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"æˆåŠŸè¯·æ±‚æ•°: {stats['successful_requests']}")
    print(f"å¤±è´¥è¯·æ±‚æ•°: {stats['failed_requests']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}ç§’")
    print(f"æœ€å°å“åº”æ—¶é—´: {stats['min_response_time']:.3f}ç§’")
    print(f"æœ€å¤§å“åº”æ—¶é—´: {stats['max_response_time']:.3f}ç§’")

async def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸŒŠ LangChain Challenge 7: æµå¼å¤„ç†å’Œå¼‚æ­¥æ“ä½œ")
        print("=" * 60)
        
        # åŸºæœ¬æµå¼å¤„ç†æ¼”ç¤º
        demo_basic_streaming()
        
        # å®æ—¶èŠå¤©æ¼”ç¤º
        demo_real_time_chat()
        
        print("\\n" + "=" * 60)
        print("âš¡ å¼‚æ­¥å¤„ç†æ¼”ç¤º...")
        
        # å¼‚æ­¥å¤„ç†æ¼”ç¤º
        await demo_async_processing()
        
        print("\\n" + "=" * 60)
        print("ğŸ”„ æµå¼æ•°æ®ç®¡é“æ¼”ç¤º...")
        
        # æµå¼æ•°æ®ç®¡é“æ¼”ç¤º
        await demo_streaming_data_pipeline()
        
        print("\\n" + "=" * 60)
        print("ğŸ“ˆ æ€§èƒ½ç›‘æ§æ¼”ç¤º...")
        
        # æ€§èƒ½ç›‘æ§æ¼”ç¤º
        demo_performance_monitoring()
        
        print("\\n" + "=" * 60)
        print("ğŸ¯ ç»ƒä¹ ä»»åŠ¡:")
        print("1. å®ç°WebSocketæ”¯æŒçš„å®æ—¶èŠå¤©æ¥å£")
        print("2. æ·»åŠ æµå¼å¤„ç†çš„é€Ÿç‡é™åˆ¶å’ŒèƒŒå‹æ§åˆ¶")
        print("3. å®ç°åˆ†å¸ƒå¼å¼‚æ­¥å¤„ç†ç³»ç»Ÿ")
        print("4. æ·»åŠ æ›´è¯¦ç»†çš„æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦")
        print("5. å®ç°æµå¼æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨")
        print("6. åˆ›å»ºå¯è§†åŒ–çš„å®æ—¶ç›‘æ§é¢æ¿")
        print("7. å®ç°å®¹é”™å’Œæ•…éšœæ¢å¤æœºåˆ¶")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\\nè¯·ç¡®ä¿:")
        print("1. å·²è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("2. å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…: pip install langchain langchain-openai aiohttp")

if __name__ == "__main__":
    asyncio.run(main())
