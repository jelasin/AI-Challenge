# -*- coding: utf-8 -*-
"""
Challenge 4: æ–‡æ¡£å¤„ç†å’ŒRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
éš¾åº¦ï¼šä¸­çº§åˆ°é«˜çº§

å­¦ä¹ ç›®æ ‡ï¼š
1. æŒæ¡Document Loaderçš„ä½¿ç”¨
2. å­¦ä¹ Text Splitterçš„å¤šç§ç­–ç•¥
3. å®ç°Embeddingå’ŒVector Store
4. æ„å»ºRetrieverç³»ç»Ÿ
5. å®ç°å®Œæ•´çš„RAGåº”ç”¨

ä»»åŠ¡æè¿°ï¼š
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š
1. å¤„ç†å¤šç§æ ¼å¼çš„æ–‡æ¡£ï¼ˆPDFã€TXTã€Markdownã€CSVç­‰ï¼‰
2. ä½¿ç”¨å¤šç§æ–‡æœ¬åˆ‡åˆ†ç­–ç•¥
3. æ„å»ºå‘é‡æ•°æ®åº“
4. å®ç°æ™ºèƒ½æ£€ç´¢
5. ç»“åˆLLMç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import (
    TextLoader, 
    PyPDFLoader,
    CSVLoader,
    UnstructuredMarkdownLoader,
    DirectoryLoader
)
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    MarkdownHeaderTextSplitter,
    TokenTextSplitter
)
from langchain_community.vectorstores import FAISS
from langchain_core.retrievers import BaseRetriever
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import os
import tempfile
import json

class DocumentAnalysis(BaseModel):
    """æ–‡æ¡£åˆ†æç»“æœ"""
    total_documents: int = Field(description="æ–‡æ¡£æ€»æ•°")
    total_chunks: int = Field(description="æ–‡æ¡£å—æ€»æ•°")
    average_chunk_length: float = Field(description="å¹³å‡å—é•¿åº¦")
    document_types: Dict[str, int] = Field(description="æ–‡æ¡£ç±»å‹ç»Ÿè®¡")
    key_topics: List[str] = Field(description="å…³é”®ä¸»é¢˜")

class QAResult(BaseModel):
    """é—®ç­”ç»“æœ"""
    answer: str = Field(description="å›ç­”")
    confidence: float = Field(description="ç½®ä¿¡åº¦", ge=0.0, le=1.0)
    sources: List[str] = Field(description="æ¥æºæ–‡æ¡£")
    relevant_chunks: List[str] = Field(description="ç›¸å…³æ–‡æ¡£å—")

def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•"""
    docs = []
    
    # åˆ›å»ºAIç›¸å…³æ–‡æ¡£
    ai_doc = """
    # äººå·¥æ™ºèƒ½åŸºç¡€

    ## æœºå™¨å­¦ä¹ 
    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
    æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡è®­ç»ƒæ•°æ®å»ºç«‹æ•°å­¦æ¨¡å‹ï¼Œä»¥ä¾¿å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ã€‚

    ### ç›‘ç£å­¦ä¹ 
    ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ ä»è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å‡½æ•°ã€‚
    å¸¸è§çš„ç›‘ç£å­¦ä¹ ç®—æ³•åŒ…æ‹¬ï¼š
    - çº¿æ€§å›å½’
    - é€»è¾‘å›å½’  
    - å†³ç­–æ ‘
    - éšæœºæ£®æ—
    - æ”¯æŒå‘é‡æœº

    ### æ— ç›‘ç£å­¦ä¹ 
    æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼æˆ–ç»“æ„ã€‚
    ä¸»è¦ç±»å‹åŒ…æ‹¬ï¼š
    - èšç±»åˆ†æ
    - é™ç»´
    - å…³è”è§„åˆ™å­¦ä¹ 

    ## æ·±åº¦å­¦ä¹ 
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘å¤„ç†ä¿¡æ¯çš„æ–¹å¼ã€‚
    æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

    ### ç¥ç»ç½‘ç»œæ¶æ„
    - å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šä¸»è¦ç”¨äºå›¾åƒå¤„ç†
    - å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼šé€‚åˆåºåˆ—æ•°æ®
    - é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ï¼šè§£å†³RNNçš„é•¿æœŸä¾èµ–é—®é¢˜
    - Transformerï¼šé©å‘½æ€§çš„æ³¨æ„åŠ›æœºåˆ¶æ¶æ„
    """
    
    # åˆ›å»ºç¼–ç¨‹ç›¸å…³æ–‡æ¡£
    programming_doc = """
    # Pythonç¼–ç¨‹æŒ‡å—

    ## åŸºç¡€è¯­æ³•
    Pythonæ˜¯ä¸€ç§é«˜çº§ã€è§£é‡Šå‹çš„ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚

    ### å˜é‡å’Œæ•°æ®ç±»å‹
    Pythonä¸­çš„åŸºæœ¬æ•°æ®ç±»å‹åŒ…æ‹¬ï¼š
    - æ•´æ•°ï¼ˆintï¼‰
    - æµ®ç‚¹æ•°ï¼ˆfloatï¼‰
    - å­—ç¬¦ä¸²ï¼ˆstrï¼‰
    - å¸ƒå°”å€¼ï¼ˆboolï¼‰
    - åˆ—è¡¨ï¼ˆlistï¼‰
    - å­—å…¸ï¼ˆdictï¼‰

    ### æ§åˆ¶ç»“æ„
    Pythonæä¾›äº†å¤šç§æ§åˆ¶ç¨‹åºæµç¨‹çš„ç»“æ„ï¼š

    #### æ¡ä»¶è¯­å¥
    ```python
    if condition:
        # æ‰§è¡Œä»£ç 
    elif another_condition:
        # æ‰§è¡Œå…¶ä»–ä»£ç 
    else:
        # é»˜è®¤æ‰§è¡Œ
    ```

    #### å¾ªç¯è¯­å¥
    ```python
    # forå¾ªç¯
    for item in iterable:
        # å¤„ç†æ¯ä¸ªå…ƒç´ 

    # whileå¾ªç¯  
    while condition:
        # é‡å¤æ‰§è¡Œ
    ```

    ## é¢å‘å¯¹è±¡ç¼–ç¨‹
    Pythonæ”¯æŒé¢å‘å¯¹è±¡ç¼–ç¨‹èŒƒå¼ï¼Œå…è®¸åˆ›å»ºç±»å’Œå¯¹è±¡ã€‚

    ### ç±»çš„å®šä¹‰
    ```python
    class MyClass:
        def __init__(self, value):
            self.value = value
        
        def method(self):
            return self.value * 2
    ```

    ## å¸¸ç”¨åº“
    Pythonæœ‰ä¸°å¯Œçš„æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“ï¼š
    - NumPyï¼šç§‘å­¦è®¡ç®—
    - Pandasï¼šæ•°æ®åˆ†æ
    - Matplotlibï¼šæ•°æ®å¯è§†åŒ–
    - Requestsï¼šHTTPè¯·æ±‚
    - Djangoï¼šWebæ¡†æ¶
    - Flaskï¼šè½»é‡çº§Webæ¡†æ¶
    """
    
    # åˆ›å»ºæ•°æ®ç§‘å­¦æ–‡æ¡£
    datascience_doc = """
    # æ•°æ®ç§‘å­¦å…¥é—¨

    ## ä»€ä¹ˆæ˜¯æ•°æ®ç§‘å­¦
    æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œä½¿ç”¨ç§‘å­¦æ–¹æ³•ã€è¿‡ç¨‹ã€ç®—æ³•å’Œç³»ç»Ÿä»ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®ä¸­æå–çŸ¥è¯†å’Œæ´å¯Ÿã€‚

    ## æ•°æ®ç§‘å­¦æµç¨‹
    å…¸å‹çš„æ•°æ®ç§‘å­¦é¡¹ç›®åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

    ### 1. é—®é¢˜å®šä¹‰
    - æ˜ç¡®ä¸šåŠ¡ç›®æ ‡
    - å®šä¹‰æˆåŠŸæŒ‡æ ‡
    - ç¡®å®šæ•°æ®éœ€æ±‚

    ### 2. æ•°æ®æ”¶é›†
    æ•°æ®å¯èƒ½æ¥è‡ªå¤šä¸ªæ¥æºï¼š
    - æ•°æ®åº“
    - APIæ¥å£
    - æ–‡ä»¶ç³»ç»Ÿ
    - ç½‘é¡µæŠ“å–
    - ä¼ æ„Ÿå™¨æ•°æ®

    ### 3. æ•°æ®æ¢ç´¢å’Œæ¸…ç†
    - æ•°æ®è´¨é‡æ£€æŸ¥
    - å¤„ç†ç¼ºå¤±å€¼
    - å¼‚å¸¸å€¼æ£€æµ‹
    - æ•°æ®ç±»å‹è½¬æ¢
    - ç‰¹å¾å·¥ç¨‹

    ### 4. å»ºæ¨¡å’Œåˆ†æ
    - é€‰æ‹©åˆé€‚çš„ç®—æ³•
    - è®­ç»ƒæ¨¡å‹
    - æ¨¡å‹éªŒè¯
    - è¶…å‚æ•°è°ƒä¼˜

    ### 5. ç»“æœå±•ç¤º
    - æ•°æ®å¯è§†åŒ–
    - æŠ¥å‘Šç¼–å†™
    - æ¨¡å‹éƒ¨ç½²
    - ç›‘æ§ç»´æŠ¤

    ## å¸¸ç”¨å·¥å…·
    - Python/Rï¼šç¼–ç¨‹è¯­è¨€
    - Jupyter Notebookï¼šäº¤äº’å¼ç¯å¢ƒ
    - Pandasï¼šæ•°æ®å¤„ç†
    - Scikit-learnï¼šæœºå™¨å­¦ä¹ 
    - TensorFlow/PyTorchï¼šæ·±åº¦å­¦ä¹ 
    - Tableau/PowerBIï¼šå¯è§†åŒ–å·¥å…·
    """
    
    return [
        ("ai_basics.md", ai_doc),
        ("python_guide.md", programming_doc),
        ("data_science.md", datascience_doc)
    ]

def create_document_loaders():
    """åˆ›å»ºå¤šç§æ–‡æ¡£åŠ è½½å™¨"""
    print("ğŸ“ åˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    sample_docs = create_sample_documents()
    file_paths = []
    
    for filename, content in sample_docs:
        file_path = os.path.join(temp_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        file_paths.append(file_path)
        print(f"åˆ›å»ºæ–‡ä»¶: {filename}")
    
    # åˆ›å»ºCSVç¤ºä¾‹
    csv_content = """name,age,occupation,description
Alice,25,Data Scientist,ä¸“é—¨ä»äº‹æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æ
Bob,30,Software Engineer,ä¸“æ³¨äºPythonå’ŒWebå¼€å‘
Carol,28,Product Manager,è´Ÿè´£AIäº§å“çš„è§„åˆ’å’Œç®¡ç†
David,32,Research Scientist,åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸæœ‰ä¸°å¯Œç»éªŒ"""
    
    csv_path = os.path.join(temp_dir, "team.csv")
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write(csv_content)
    file_paths.append(csv_path)
    print("åˆ›å»ºæ–‡ä»¶: team.csv")
    
    return temp_dir, file_paths

def demo_text_splitters(documents: List[Document]):
    """æ¼”ç¤ºä¸åŒçš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥"""
    print("\nğŸ“ æ–‡æœ¬åˆ†å‰²ç­–ç•¥æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹ç”¨äºæ¼”ç¤º
    full_text = "\n\n".join([doc.page_content for doc in documents])
    
    # 1. é€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ˆæ¨èï¼‰
    print("1. é€’å½’å­—ç¬¦åˆ†å‰²å™¨:")
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""]
    )
    recursive_chunks = recursive_splitter.split_text(full_text)
    print(f"   ç”Ÿæˆå—æ•°: {len(recursive_chunks)}")
    print(f"   ç¬¬ä¸€å—é•¿åº¦: {len(recursive_chunks[0])}")
    
    # 2. Markdownå¤´éƒ¨åˆ†å‰²å™¨
    print("\n2. Markdownå¤´éƒ¨åˆ†å‰²å™¨:")
    md_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=[
            ("#", "Header 1"),
            ("##", "Header 2"), 
            ("###", "Header 3"),
        ]
    )
    # åªå¤„ç†Markdownæ–‡æ¡£
    md_documents = [doc for doc in documents if doc.metadata.get('source', '').endswith('.md')]
    if md_documents:
        md_chunks = md_splitter.split_text(md_documents[0].page_content)
        print(f"   ç”Ÿæˆå—æ•°: {len(md_chunks)}")
        if md_chunks:
            print(f"   ç¬¬ä¸€å—å…ƒæ•°æ®: {md_chunks[0].metadata}")
    
    # 3. Tokenåˆ†å‰²å™¨
    print("\n3. Tokenåˆ†å‰²å™¨:")
    token_splitter = TokenTextSplitter(
        chunk_size=200,
        chunk_overlap=20
    )
    token_chunks = token_splitter.split_text(full_text)
    print(f"   ç”Ÿæˆå—æ•°: {len(token_chunks)}")
    print(f"   ç¬¬ä¸€å—é•¿åº¦: {len(token_chunks[0])}")
    
    return recursive_chunks

def create_rag_system():
    """åˆ›å»ºRAGç³»ç»Ÿ"""
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    
    print("\nğŸ”§ æ„å»ºRAGç³»ç»Ÿ...")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    temp_dir, file_paths = create_document_loaders()
    
    # åŠ è½½æ‰€æœ‰æ–‡æ¡£
    documents = []
    
    # åŠ è½½Markdownæ–‡ä»¶
    for file_path in file_paths:
        if file_path.endswith('.md'):
            loader = TextLoader(file_path, encoding='utf-8')
            docs = loader.load()
            for doc in docs:
                doc.metadata['source'] = os.path.basename(file_path)
                doc.metadata['type'] = 'markdown'
            documents.extend(docs)
        elif file_path.endswith('.csv'):
            loader = CSVLoader(file_path, encoding='utf-8')
            docs = loader.load()
            for doc in docs:
                doc.metadata['source'] = os.path.basename(file_path)
                doc.metadata['type'] = 'csv'
            documents.extend(docs)
    
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æ¼”ç¤ºæ–‡æœ¬åˆ†å‰²
    chunks = demo_text_splitters(documents)
    
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    
    # åˆ†å‰²æ–‡æ¡£
    split_documents = text_splitter.split_documents(documents)
    print(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(split_documents)} ä¸ªå—")
    
    # åˆ›å»ºåµŒå…¥æ¨¡å‹
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"  # ä½¿ç”¨è¾ƒæ–°çš„åµŒå…¥æ¨¡å‹
    )
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    print("ğŸ”„ åˆ›å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = FAISS.from_documents(split_documents, embeddings)
    print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}  # æ£€ç´¢æœ€ç›¸ä¼¼çš„4ä¸ªå—
    )
    
    # åˆ›å»ºLLM
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.1
    )
    
    # åˆ›å»ºRAG Prompt
    rag_prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

å›ç­”ï¼š""")
    
    # åˆ›å»ºRAGé“¾
    rag_chain = (
        {"context": retriever | format_documents, "question": RunnablePassthrough()}
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain, retriever, vectorstore, len(split_documents)

def format_documents(docs):
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    return "\n\n".join([f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\nå†…å®¹: {doc.page_content}" for doc in docs])

def demo_advanced_retrieval(retriever, vectorstore):
    """æ¼”ç¤ºé«˜çº§æ£€ç´¢åŠŸèƒ½"""
    print("\nğŸ” é«˜çº§æ£€ç´¢åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    # 1. ç›¸ä¼¼æ€§æœç´¢
    print("1. ç›¸ä¼¼æ€§æœç´¢:")
    similar_docs = vectorstore.similarity_search("æœºå™¨å­¦ä¹ ç®—æ³•", k=3)
    for i, doc in enumerate(similar_docs, 1):
        print(f"   ç»“æœ {i}: {doc.page_content[:100]}...")
        print(f"           æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
    
    # 2. ç›¸ä¼¼æ€§æœç´¢å¸¦åˆ†æ•°
    print("\n2. å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢:")
    similar_docs_with_scores = vectorstore.similarity_search_with_score("Pythonç¼–ç¨‹", k=3)
    for i, (doc, score) in enumerate(similar_docs_with_scores, 1):
        print(f"   ç»“æœ {i} (åˆ†æ•°: {score:.4f}): {doc.page_content[:80]}...")
    
    # TODO: å®ç°æ›´å¤šé«˜çº§æ£€ç´¢åŠŸèƒ½
    print("\nğŸ¯ å¾…å®ç°çš„é«˜çº§åŠŸèƒ½:")
    print("- æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯ + å‘é‡ï¼‰")
    print("- é‡æ’åºæ£€ç´¢")
    print("- å¤šæŸ¥è¯¢æ£€ç´¢")
    print("- ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢")

def analyze_document_collection(documents, chunks_count):
    """åˆ†ææ–‡æ¡£é›†åˆ"""
    print("\nğŸ“Š æ–‡æ¡£é›†åˆåˆ†æ")
    print("=" * 50)
    
    doc_types = {}
    total_length = 0
    
    for doc in documents:
        doc_type = doc.metadata.get('type', 'unknown')
        doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        total_length += len(doc.page_content)
    
    avg_length = total_length / len(documents) if documents else 0
    
    analysis = DocumentAnalysis(
        total_documents=len(documents),
        total_chunks=chunks_count,
        average_chunk_length=avg_length,
        document_types=doc_types,
        key_topics=["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "Pythonç¼–ç¨‹", "æ•°æ®ç§‘å­¦"]  # ç®€åŒ–çš„ä¸»é¢˜æå–
    )
    
    print(f"ğŸ“‹ æ–‡æ¡£ç»Ÿè®¡:")
    print(f"   æ€»æ–‡æ¡£æ•°: {analysis.total_documents}")
    print(f"   æ€»å—æ•°: {analysis.total_chunks}")
    print(f"   å¹³å‡æ–‡æ¡£é•¿åº¦: {analysis.average_chunk_length:.0f} å­—ç¬¦")
    print(f"   æ–‡æ¡£ç±»å‹: {analysis.document_types}")
    print(f"   å…³é”®ä¸»é¢˜: {', '.join(analysis.key_topics)}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ” LangChain Challenge 4: æ–‡æ¡£å¤„ç†å’ŒRAG")
        print("=" * 60)
        
        # åˆ›å»ºRAGç³»ç»Ÿ
        rag_chain, retriever, vectorstore, chunks_count = create_rag_system()
        
        # åˆ†ææ–‡æ¡£é›†åˆï¼ˆè¿™é‡Œæˆ‘ä»¬éœ€è¦é‡æ–°åŠ è½½documentsæ¥åˆ†æï¼‰
        temp_dir, file_paths = create_document_loaders()
        documents = []
        for file_path in file_paths:
            if file_path.endswith('.md'):
                loader = TextLoader(file_path, encoding='utf-8')
                documents.extend(loader.load())
        
        analyze_document_collection(documents, chunks_count)
        
        # æ¼”ç¤ºé«˜çº§æ£€ç´¢
        demo_advanced_retrieval(retriever, vectorstore)
        
        print("\n" + "=" * 60)
        print("ğŸ’¬ å¼€å§‹é—®ç­”æ¼”ç¤º...")
        
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿå®ƒæœ‰å“ªäº›ä¸»è¦ç±»å‹ï¼Ÿ",
            "Pythonä¸­æœ‰å“ªäº›åŸºæœ¬çš„æ•°æ®ç±»å‹ï¼Ÿ",
            "æ•°æ®ç§‘å­¦çš„å…¸å‹æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "å›¢é˜Ÿä¸­æœ‰å“ªäº›æˆå‘˜ï¼Ÿä»–ä»¬çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nâ“ é—®é¢˜ {i}: {question}")
            print("-" * 40)
            
            # è·å–ç­”æ¡ˆ
            answer = rag_chain.invoke(question)
            print(f"ğŸ¤– å›ç­”: {answer}")
            
            # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£
            relevant_docs = retriever.invoke(question)
            print(f"\nğŸ“– ç›¸å…³æ–‡æ¡£å— ({len(relevant_docs)} ä¸ª):")
            for j, doc in enumerate(relevant_docs, 1):
                print(f"   {j}. æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
                print(f"      å†…å®¹: {doc.page_content[:150]}...")
            
            if i < len(test_questions):  # ä¸æ˜¯æœ€åä¸€ä¸ªé—®é¢˜
                print()
        
        print("\n" + "=" * 60)
        print("ğŸ¯ ç»ƒä¹ ä»»åŠ¡:")
        print("1. å®ç°å¤šç§æ–‡æ¡£æ ¼å¼çš„åŠ è½½å™¨ï¼ˆPDFã€Wordã€Excelç­‰ï¼‰")
        print("2. æ·»åŠ æ–‡æ¡£å…ƒæ•°æ®å¢å¼ºï¼ˆæ—¶é—´æˆ³ã€ä½œè€…ã€æ ‡ç­¾ç­‰ï¼‰")
        print("3. å®ç°æ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡æ£€ç´¢ï¼‰")
        print("4. æ·»åŠ æ£€ç´¢ç»“æœé‡æ’åºåŠŸèƒ½")
        print("5. å®ç°æ–‡æ¡£æ›´æ–°å’Œå¢é‡ç´¢å¼•")
        print("6. æ·»åŠ æŸ¥è¯¢æ‰©å±•å’Œæ„å›¾ç†è§£")
        print("7. å®ç°å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ç®¡ç†")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("1. å·²è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("2. å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…:")
        print("   pip install langchain langchain-openai langchain-community")
        print("   pip install faiss-cpu pypdf unstructured")

if __name__ == "__main__":
    main()
