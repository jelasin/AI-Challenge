# -*- coding: utf-8 -*-
"""
Challenge 4: æ–‡æ¡£å¤„ç†å’ŒRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰v0.3
éš¾åº¦ï¼šä¸­çº§åˆ°é«˜çº§

å­¦ä¹ ç›®æ ‡ï¼š
1. æŒæ¡Document Loaderçš„ä½¿ç”¨
2. å­¦ä¹ Text Splitterçš„å¤šç§ç­–ç•¥
3. å®ç°Embeddingå’ŒVector Store
4. æ„å»ºRetrieverç³»ç»Ÿ
5. å®ç°å®Œæ•´çš„RAGåº”ç”¨

ä»»åŠ¡æè¿°ï¼š
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š
1. å¤„ç†å¤šç§æ ¼å¼çš„æ–‡æ¡£ï¼ˆTXTã€Markdownã€CSVç­‰ï¼‰
2. ä½¿ç”¨å¤šç§æ–‡æœ¬åˆ‡åˆ†ç­–ç•¥
3. æ„å»ºå‘é‡æ•°æ®åº“
4. å®ç°æ™ºèƒ½æ£€ç´¢
5. ç»“åˆLLMç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ
6. è®°å½•å†å²å¯¹è¯
"""

from langchain_openai import OpenAIEmbeddings
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    CSVLoader,
    DirectoryLoader,
)
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
    TokenTextSplitter
)
from langchain_community.vectorstores import FAISS
from pydantic import BaseModel, Field
from typing import List,  Dict
import os
from pathlib import Path
from operator import itemgetter

class DocumentAnalysis(BaseModel):
    """æ–‡æ¡£åˆ†æç»“æœ"""
    total_documents: int = Field(description="æ–‡æ¡£æ€»æ•°")
    total_chunks: int = Field(description="æ–‡æ¡£å—æ€»æ•°")
    average_chunk_length: float = Field(description="å¹³å‡å—é•¿åº¦")
    document_types: Dict[str, int] = Field(description="æ–‡æ¡£ç±»å‹ç»Ÿè®¡")
    key_topics: List[str] = Field(description="å…³é”®ä¸»é¢˜")

class QAResult(BaseModel):
    """é—®ç­”ç»“æœ"""
    answer: str = Field(description="å›ç­”")
    confidence: float = Field(description="ç½®ä¿¡åº¦", ge=0.0, le=1.0)
    sources: List[str] = Field(description="æ¥æºæ–‡æ¡£")
    relevant_chunks: List[str] = Field(description="ç›¸å…³æ–‡æ¡£å—")

def load_documents_from_dir(doc_dir: str) -> list[Document]:
    """ä»æŒ‡å®šç›®å½•åŠ è½½å¤šç§æ ¼å¼æ–‡æ¡£ï¼ˆmd/txt/csv/pdfï¼‰ã€‚"""
    print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½æ–‡æ¡£: {doc_dir}")
    documents: list[Document] = []

    # Markdown ä¸ TXT
    md_loader = DirectoryLoader(
        doc_dir, glob="**/*.md", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}
    )
    txt_loader = DirectoryLoader(
        doc_dir, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"}
    )
    # CSVï¼ˆæ¯è¡Œä¸€æ¡ Documentï¼‰
    csv_loader = DirectoryLoader(
        doc_dir, glob="**/*.csv", loader_cls=CSVLoader, loader_kwargs={"encoding": "utf-8"}
    )
    for loader, dtype in (
        (md_loader, "markdown"),
        (txt_loader, "text"),
        (csv_loader, "csv"),
    ):
        try:
            docs = loader.load()
        except Exception as e:
            # æŸäº› loader å¯èƒ½å› ä¾èµ–ç¼ºå¤±å¤±è´¥ï¼Œè·³è¿‡å¹¶æç¤º
            print(f"âš ï¸ åŠ è½½ {dtype} æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            docs = []
        for d in docs:
            d.metadata["source"] = os.path.relpath(d.metadata.get("source", d.metadata.get("file_path", "")), doc_dir) if d.metadata else ""
            d.metadata["type"] = dtype
        documents.extend(docs)

    print(f"âœ… å…±åŠ è½½æ–‡æ¡£ {len(documents)} æ¡")
    return documents

def demo_text_splitters(documents: List[Document]):
    """æ¼”ç¤ºä¸åŒçš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥"""
    print("\nğŸ“ æ–‡æœ¬åˆ†å‰²ç­–ç•¥æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹ç”¨äºæ¼”ç¤º
    full_text = "\n\n".join([doc.page_content for doc in documents])
    
    # 1. é€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ˆæ¨èï¼‰
    print("1. é€’å½’å­—ç¬¦åˆ†å‰²å™¨:")
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""]
    )
    recursive_chunks = recursive_splitter.split_text(full_text)
    print(f"   ç”Ÿæˆå—æ•°: {len(recursive_chunks)}")
    if recursive_chunks:
        print(f"   ç¬¬ä¸€å—é•¿åº¦: {len(recursive_chunks[0])}")
    
    # 2. Markdownå¤´éƒ¨åˆ†å‰²å™¨
    print("\n2. Markdownå¤´éƒ¨åˆ†å‰²å™¨:")
    md_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=[
            ("#", "Header 1"),
            ("##", "Header 2"), 
            ("###", "Header 3"),
        ]
    )
    # åªå¤„ç†Markdownæ–‡æ¡£
    md_documents = [doc for doc in documents if doc.metadata.get('source', '').endswith('.md')]
    if md_documents:
        md_chunks = md_splitter.split_text(md_documents[0].page_content)
        print(f"   ç”Ÿæˆå—æ•°: {len(md_chunks)}")
        if md_chunks:
            print(f"   ç¬¬ä¸€å—å…ƒæ•°æ®: {md_chunks[0].metadata}")
    
    # 3. Tokenåˆ†å‰²å™¨
    print("\n3. Tokenåˆ†å‰²å™¨:")
    token_splitter = TokenTextSplitter(
        chunk_size=200,
        chunk_overlap=20
    )
    token_chunks = token_splitter.split_text(full_text)
    print(f"   ç”Ÿæˆå—æ•°: {len(token_chunks)}")
    if token_chunks:
        print(f"   ç¬¬ä¸€å—é•¿åº¦: {len(token_chunks[0])}")
    
    return recursive_chunks

def create_rag_system(doc_dir: str):
    """åˆ›å»ºRAGç³»ç»Ÿï¼ŒåŸºäºæœ¬åœ° doc ç›®å½•æ„å»ºç´¢å¼•ä¸æ£€ç´¢ã€‚"""
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")

    print("\nğŸ”§ æ„å»ºRAGç³»ç»Ÿ...")

    # ä»ç›®å½•åŠ è½½æ–‡æ¡£
    documents = load_documents_from_dir(doc_dir)
    if not documents:
        raise ValueError(f"åœ¨ç›®å½• {doc_dir} ä¸‹æœªå‘ç°å¯åŠ è½½çš„æ–‡æ¡£ï¼ˆæ”¯æŒ md/txt/csv/pdfï¼‰ã€‚")
    
    # æ¼”ç¤ºæ–‡æœ¬åˆ†å‰²
    chunks = demo_text_splitters(documents)
    
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    
    # åˆ†å‰²æ–‡æ¡£
    split_documents = text_splitter.split_documents(documents)
    print(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(split_documents)} ä¸ªå—")
    
    # åˆ›å»ºåµŒå…¥æ¨¡å‹
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"  # ä½¿ç”¨è¾ƒæ–°çš„åµŒå…¥æ¨¡å‹
    )
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    print("ğŸ”„ åˆ›å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = FAISS.from_documents(split_documents, embeddings)
    print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}  # æ£€ç´¢æœ€ç›¸ä¼¼çš„4ä¸ªå—
    )
    
    # åˆ›å»ºLLMï¼ˆv0.3 æ¨èï¼šinit_chat_modelï¼‰
    llm = init_chat_model("gpt-4o", temperature=0.1)
    
    # åˆ›å»ºRAG Prompt
    rag_prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ç»¼åˆâ€œæœ€è¿‘å¯¹è¯å†å²â€å’Œâ€œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡â€æ¥å›ç­”å½“å‰ç”¨æˆ·é—®é¢˜ï¼›è‹¥ä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜æ— æ³•ä»æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

æœ€è¿‘å¯¹è¯å†å²ï¼ˆæœ€å¤š5è½®ï¼Œè‹¥ä¸ºç©ºå¯å¿½ç•¥ï¼‰ï¼š
{chat_history}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”ï¼š""")
    
    # åˆ›å»ºRAGé“¾
    # é“¾è¾“å…¥ï¼š{"question": str, "chat_history": str}
    rag_chain = (
        {
            "context": itemgetter("question") | retriever | format_documents,
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        }
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain, retriever, vectorstore, len(split_documents), documents

def format_documents(docs):
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    return "\n\n".join([f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\nå†…å®¹: {doc.page_content}" for doc in docs])

def demo_advanced_retrieval(retriever, vectorstore):
    """æ¼”ç¤ºé«˜çº§æ£€ç´¢åŠŸèƒ½"""
    print("\nğŸ” é«˜çº§æ£€ç´¢åŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    # 1. ç›¸ä¼¼æ€§æœç´¢
    print("1. ç›¸ä¼¼æ€§æœç´¢:")
    similar_docs = vectorstore.similarity_search("æœºå™¨å­¦ä¹ ç®—æ³•", k=3)
    for i, doc in enumerate(similar_docs, 1):
        print(f"   ç»“æœ {i}: {doc.page_content[:100]}...")
        print(f"           æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
    
    # 2. ç›¸ä¼¼æ€§æœç´¢å¸¦åˆ†æ•°
    print("\n2. å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢:")
    similar_docs_with_scores = vectorstore.similarity_search_with_score("Pythonç¼–ç¨‹", k=3)
    for i, (doc, score) in enumerate(similar_docs_with_scores, 1):
        print(f"   ç»“æœ {i} (åˆ†æ•°: {score:.4f}): {doc.page_content[:80]}...")
    
    # TODO: å®ç°æ›´å¤šé«˜çº§æ£€ç´¢åŠŸèƒ½
    print("\nğŸ¯ å¾…å®ç°çš„é«˜çº§åŠŸèƒ½:")
    print("- æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯ + å‘é‡ï¼‰")
    print("- é‡æ’åºæ£€ç´¢")
    print("- å¤šæŸ¥è¯¢æ£€ç´¢")
    print("- ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢")

def analyze_document_collection(documents, chunks_count):
    """åˆ†ææ–‡æ¡£é›†åˆ"""
    print("\nğŸ“Š æ–‡æ¡£é›†åˆåˆ†æ")
    print("=" * 50)
    
    doc_types = {}
    total_length = 0
    
    for doc in documents:
        doc_type = doc.metadata.get('type', 'unknown')
        doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        total_length += len(doc.page_content)
    
    avg_length = total_length / len(documents) if documents else 0
    
    analysis = DocumentAnalysis(
        total_documents=len(documents),
        total_chunks=chunks_count,
        average_chunk_length=avg_length,
        document_types=doc_types,
        key_topics=["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "Pythonç¼–ç¨‹", "æ•°æ®ç§‘å­¦"]  # ç®€åŒ–çš„ä¸»é¢˜æå–
    )
    
    print(f"ğŸ“‹ æ–‡æ¡£ç»Ÿè®¡:")
    print(f"   æ€»æ–‡æ¡£æ•°: {analysis.total_documents}")
    print(f"   æ€»å—æ•°: {analysis.total_chunks}")
    print(f"   å¹³å‡æ–‡æ¡£é•¿åº¦: {analysis.average_chunk_length:.0f} å­—ç¬¦")
    print(f"   æ–‡æ¡£ç±»å‹: {analysis.document_types}")
    print(f"   å…³é”®ä¸»é¢˜: {', '.join(analysis.key_topics)}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ” LangChain Challenge 4: æ–‡æ¡£å¤„ç†å’ŒRAG")
        print("=" * 60)
        # æ–‡æ¡£ç›®å½•ï¼ˆç›¸å¯¹å½“å‰æ–‡ä»¶ï¼‰
        docs_dir = str(Path(__file__).parent.joinpath("doc").resolve())

        # åˆ›å»ºRAGç³»ç»Ÿ
        rag_chain, retriever, vectorstore, chunks_count, documents = create_rag_system(docs_dir)

        # åˆ†ææ–‡æ¡£é›†åˆ
        analyze_document_collection(documents, chunks_count)

        # æ¼”ç¤ºé«˜çº§æ£€ç´¢
        demo_advanced_retrieval(retriever, vectorstore)

        print("\n" + "=" * 60)
        print("ğŸ’¬ äº¤äº’é—®ç­”æ¨¡å¼ï¼ˆè¾“å…¥å†…å®¹åå›è½¦ï¼‰ï¼š")
        print("   - è¾“å…¥ exit / quit / q æˆ–ç›´æ¥å›è½¦å¯é€€å‡ºã€‚")

        # äº¤äº’å¼é—®ç­”å¾ªç¯
        history: list[tuple[str, str]] = []  # è®°å½•æœ€è¿‘ 5 è½® (question, answer)
        while True:
            try:
                question = input("\nâ“ ä½ çš„é—®é¢˜: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ é€€å‡ºã€‚")
                break

            if not question or question.lower() in {"exit", "quit", "q"}:
                print("ğŸ‘‹ å·²é€€å‡ºé—®ç­”æ¨¡å¼ã€‚")
                break

            print("-" * 40)
            # ç»„è£…æœ€è¿‘ 5 è½®å¯¹è¯å†å²
            if history:
                history_text = "\n".join([f"ç”¨æˆ·ï¼š{q}\nåŠ©æ‰‹ï¼š{a}" for q, a in history[-5:]])
            else:
                history_text = "ï¼ˆæ— ï¼‰"

            # è·å–ç­”æ¡ˆï¼ˆä¼ å…¥é—®é¢˜ä¸å†å²ï¼‰
            payload = {"question": question, "chat_history": history_text}
            answer = rag_chain.invoke(payload)
            print(f"ğŸ¤– å›ç­”: {answer}")

            # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£
            relevant_docs = retriever.invoke(question)
            print(f"\nğŸ“– ç›¸å…³æ–‡æ¡£å— ({len(relevant_docs)} ä¸ª):")
            for j, doc in enumerate(relevant_docs, 1):
                print(f"   {j}. æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
                print(f"      å†…å®¹: {doc.page_content[:150]}...")

            # æ›´æ–°å¯¹è¯å†å²ï¼Œæœ€å¤šä¿ç•™ 5 è½®
            history.append((question, answer))
            if len(history) > 5:
                history = history[-5:]

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("1. å·²è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("2. å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åŒ…:")
        print("   pip install langchain langchain-openai langchain-community")
        print("   pip install faiss-cpu pypdf unstructured")

if __name__ == "__main__":
    main()
