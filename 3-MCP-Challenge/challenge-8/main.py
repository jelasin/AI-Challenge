#!/usr/bin/env python3
"""
3-MCP-Challenge - Challenge 8: ç»¼åˆåº”ç”¨ï¼šæ™ºèƒ½å·¥ä½œæµå¼•æ“
éš¾åº¦ï¼šâ­â­â­â­â­â­â­â­

å­¦ä¹ ç›®æ ‡ï¼š
1. å¤æ‚å·¥ä½œæµè®¾è®¡ä¸æ‰§è¡Œ
2. å¤šæ¨¡æ€æ•°æ®å¤„ç†
3. æ™ºèƒ½å†³ç­–å¼•æ“
4. å·¥ä½œæµå¯è§†åŒ–
5. é”™è¯¯æ¢å¤ä¸é‡è¯•æœºåˆ¶
6. æ€§èƒ½ä¼˜åŒ–ä¸ç¼“å­˜
7. å®æ—¶ç›‘æ§ä¸å‘Šè­¦

å‚è€ƒé“¾æ¥ï¼š
- MCP Workflow Patterns: https://modelcontextprotocol.io/docs/workflow-patterns
- MCP Multi-Modal: https://modelcontextprotocol.io/docs/multi-modal
- MCP Performance: https://modelcontextprotocol.io/docs/performance
"""

import os
import json
import time
import asyncio
import hashlib
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict, deque
import sqlite3
import uuid
import base64
from pathlib import Path

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict, Annotated

from fastmcp import FastMCP
from mcp import types


# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class WorkflowStatus(Enum):
    """å·¥ä½œæµçŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"
    CANCELLED = "cancelled"


class NodeType(Enum):
    """èŠ‚ç‚¹ç±»å‹æšä¸¾"""
    INPUT = "input"
    PROCESSING = "processing"
    DECISION = "decision"
    OUTPUT = "output"
    CONDITION = "condition"
    LOOP = "loop"
    PARALLEL = "parallel"


@dataclass
class WorkflowNode:
    """å·¥ä½œæµèŠ‚ç‚¹"""
    id: str
    name: str
    node_type: NodeType
    config: Dict[str, Any] = field(default_factory=dict)
    inputs: List[str] = field(default_factory=list)
    outputs: List[str] = field(default_factory=list)
    conditions: Dict[str, str] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 3
    timeout: float = 300.0
    
    def __post_init__(self):
        if not self.id:
            self.id = str(uuid.uuid4())


@dataclass
class WorkflowEdge:
    """å·¥ä½œæµè¿æ¥è¾¹"""
    from_node: str
    to_node: str
    condition: Optional[str] = None
    weight: float = 1.0


@dataclass
class WorkflowExecution:
    """å·¥ä½œæµæ‰§è¡Œè®°å½•"""
    id: str
    workflow_id: str
    status: WorkflowStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    input_data: Dict[str, Any] = field(default_factory=dict)
    output_data: Dict[str, Any] = field(default_factory=dict)
    execution_log: List[Dict[str, Any]] = field(default_factory=list)
    error_message: Optional[str] = None
    
    def __post_init__(self):
        if not self.id:
            self.id = str(uuid.uuid4())


@dataclass
class WorkflowDefinition:
    """å·¥ä½œæµå®šä¹‰"""
    id: str
    name: str
    description: str
    version: str
    nodes: List[WorkflowNode]
    edges: List[WorkflowEdge]
    global_config: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    
    def __post_init__(self):
        if not self.id:
            self.id = str(uuid.uuid4())


class WorkflowState(TypedDict):
    """LangGraphå·¥ä½œæµçŠ¶æ€"""
    execution_id: str
    current_node: str
    data: Dict[str, Any]
    context: Dict[str, Any]
    errors: List[str]
    iteration_count: int
    start_time: float


class MultiModalProcessor:
    """å¤šæ¨¡æ€æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.1)
    
    async def process_text(self, content: str, task: str = "analyze") -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are an expert text processor. Analyze the given text according to the task."),
                ("human", f"Task: {task}\nContent: {content}")
            ])
            
            result = await self.llm.ainvoke(prompt.format_messages())
            return {
                "type": "text",
                "task": task,
                "input_length": len(content),
                "result": result.content,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Text processing failed: {e}")
            return {"error": str(e), "type": "text", "task": task}
    
    async def process_json(self, data: Dict[str, Any], task: str = "validate") -> Dict[str, Any]:
        """å¤„ç†JSONæ•°æ®"""
        try:
            if task == "validate":
                # éªŒè¯JSONç»“æ„
                result = {
                    "valid": True,
                    "keys_count": len(data.keys()) if isinstance(data, dict) else 0,
                    "data_type": type(data).__name__,
                    "structure": self._analyze_json_structure(data)
                }
            elif task == "transform":
                # æ•°æ®è½¬æ¢
                result = self._transform_json_data(data)
            elif task == "extract":
                # æå–å…³é”®ä¿¡æ¯
                result = self._extract_key_info(data)
            else:
                result = {"message": f"Unknown task: {task}"}
            
            return {
                "type": "json",
                "task": task,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"JSON processing failed: {e}")
            return {"error": str(e), "type": "json", "task": task}
    
    async def process_file(self, file_path: str, task: str = "analyze") -> Dict[str, Any]:
        """å¤„ç†æ–‡ä»¶æ•°æ®"""
        try:
            if not os.path.exists(file_path):
                return {"error": "File not found", "path": file_path}
            
            file_info = {
                "path": file_path,
                "size": os.path.getsize(file_path),
                "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                "extension": Path(file_path).suffix.lower()
            }
            
            if file_info["extension"] in [".txt", ".md", ".py", ".js", ".html", ".css"]:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    text_result = await self.process_text(content, task)
                    file_info.update(text_result)
            
            return {
                "type": "file",
                "task": task,
                "result": file_info,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"File processing failed: {e}")
            return {"error": str(e), "type": "file", "path": file_path}
    
    def _analyze_json_structure(self, data: Any, max_depth: int = 3, current_depth: int = 0) -> Dict[str, Any]:
        """åˆ†æJSONç»“æ„"""
        if current_depth >= max_depth:
            return {"truncated": True, "type": type(data).__name__}
        
        if isinstance(data, dict):
            return {
                "type": "object",
                "keys": list(data.keys())[:10],  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªé”®
                "key_count": len(data),
                "sample_values": {
                    k: self._analyze_json_structure(v, max_depth, current_depth + 1)
                    for k, v in list(data.items())[:3]  # åªåˆ†æå‰3ä¸ªå€¼
                }
            }
        elif isinstance(data, list):
            return {
                "type": "array",
                "length": len(data),
                "sample_items": [
                    self._analyze_json_structure(item, max_depth, current_depth + 1)
                    for item in data[:3]  # åªåˆ†æå‰3ä¸ªå…ƒç´ 
                ]
            }
        else:
            return {
                "type": type(data).__name__,
                "value": str(data)[:100] if isinstance(data, str) else data
            }
    
    def _transform_json_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢JSONæ•°æ®"""
        # è¿™é‡Œå®ç°æ•°æ®è½¬æ¢é€»è¾‘
        transformed = {}
        for key, value in data.items():
            # ç¤ºä¾‹è½¬æ¢ï¼šé©¼å³°å‘½åè½¬ä¸‹åˆ’çº¿
            new_key = self._camel_to_snake(key)
            transformed[new_key] = value
        return transformed
    
    def _camel_to_snake(self, name: str) -> str:
        """é©¼å³°å‘½åè½¬ä¸‹åˆ’çº¿å‘½å"""
        import re
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    
    def _extract_key_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯"""
        key_info = {
            "summary": f"Data contains {len(data)} top-level fields",
            "important_fields": [],
            "data_types": defaultdict(int)
        }
        
        for key, value in data.items():
            key_info["data_types"][type(value).__name__] += 1
            if any(keyword in key.lower() for keyword in ["id", "name", "title", "email", "status"]):
                key_info["important_fields"].append({
                    "field": key,
                    "type": type(value).__name__,
                    "value": str(value)[:50]
                })
        
        return key_info


class DecisionEngine:
    """æ™ºèƒ½å†³ç­–å¼•æ“"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.1)
        self.rules: List[Dict[str, Any]] = []
    
    def add_rule(self, name: str, condition: str, action: str, priority: int = 1):
        """æ·»åŠ å†³ç­–è§„åˆ™"""
        self.rules.append({
            "name": name,
            "condition": condition,
            "action": action,
            "priority": priority,
            "created_at": datetime.now()
        })
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.rules.sort(key=lambda x: x["priority"], reverse=True)
    
    async def make_decision(self, context: Dict[str, Any], 
                           question: str = None) -> Dict[str, Any]:
        """åšå‡ºæ™ºèƒ½å†³ç­–"""
        try:
            # é¦–å…ˆå°è¯•åŸºäºè§„åˆ™çš„å†³ç­–
            rule_decision = self._apply_rules(context)
            if rule_decision:
                return {
                    "type": "rule_based",
                    "decision": rule_decision,
                    "confidence": 0.9,
                    "reasoning": f"Applied rule: {rule_decision['rule_name']}"
                }
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œä½¿ç”¨LLMå†³ç­–
            if question:
                llm_decision = await self._llm_decision(context, question)
                return {
                    "type": "llm_based",
                    "decision": llm_decision,
                    "confidence": 0.7,
                    "reasoning": "Generated by language model"
                }
            
            # é»˜è®¤å†³ç­–
            return {
                "type": "default",
                "decision": {"action": "continue", "next_node": None},
                "confidence": 0.5,
                "reasoning": "No matching rules or specific question"
            }
            
        except Exception as e:
            logger.error(f"Decision making failed: {e}")
            return {
                "type": "error",
                "decision": {"action": "abort", "error": str(e)},
                "confidence": 0.0,
                "reasoning": f"Error occurred: {e}"
            }
    
    def _apply_rules(self, context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """åº”ç”¨è§„åˆ™å†³ç­–"""
        for rule in self.rules:
            try:
                # ç®€å•çš„æ¡ä»¶è¯„ä¼°ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®‰å…¨çš„è¡¨è¾¾å¼è¯„ä¼°å™¨ï¼‰
                condition = rule["condition"]
                # æ›¿æ¢ä¸Šä¸‹æ–‡å˜é‡
                for key, value in context.items():
                    condition = condition.replace(f"{{{key}}}", str(value))
                
                # è¯„ä¼°æ¡ä»¶ï¼ˆæ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨evalæ˜¯ä¸å®‰å…¨çš„ï¼Œä»…ç”¨äºæ¼”ç¤ºï¼‰
                if self._safe_eval(condition, context):
                    return {
                        "rule_name": rule["name"],
                        "action": rule["action"],
                        "matched_condition": rule["condition"]
                    }
            except Exception as e:
                logger.warning(f"Rule evaluation failed for {rule['name']}: {e}")
                continue
        
        return None
    
    def _safe_eval(self, expression: str, context: Dict[str, Any]) -> bool:
        """å®‰å…¨çš„è¡¨è¾¾å¼è¯„ä¼°"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®‰å…¨çš„è¡¨è¾¾å¼è¯„ä¼°åº“
        try:
            # åªå…è®¸ç®€å•çš„æ¯”è¾ƒæ“ä½œ
            allowed_chars = set("0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_<>=!&| ()")
            if not all(c in allowed_chars for c in expression):
                return False
            
            # åˆ›å»ºå®‰å…¨çš„è¯„ä¼°ç¯å¢ƒ
            safe_dict = {
                "__builtins__": {},
                "True": True,
                "False": False
            }
            safe_dict.update(context)
            
            return eval(expression, safe_dict)
        except:
            return False
    
    async def _llm_decision(self, context: Dict[str, Any], question: str) -> Dict[str, Any]:
        """åŸºäºLLMçš„å†³ç­–"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an intelligent decision engine. Based on the provided context and question, 
            make a decision and provide your reasoning. Respond in JSON format with the following structure:
            {
                "action": "continue|pause|retry|abort|branch",
                "next_node": "node_id or null",
                "parameters": {},
                "reasoning": "explanation of the decision"
            }"""),
            ("human", f"Context: {json.dumps(context, indent=2)}\n\nQuestion: {question}")
        ])
        
        result = await self.llm.ainvoke(prompt.format_messages())
        try:
            return json.loads(result.content)
        except:
            return {
                "action": "continue",
                "reasoning": result.content,
                "parameters": {}
            }


class WorkflowEngine:
    """å·¥ä½œæµå¼•æ“"""
    
    def __init__(self):
        self.workflows: Dict[str, WorkflowDefinition] = {}
        self.executions: Dict[str, WorkflowExecution] = {}
        self.processor = MultiModalProcessor()
        self.decision_engine = DecisionEngine()
        self.db_path = "workflow_engine.db"
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS workflows (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                definition TEXT NOT NULL,
                created_at TIMESTAMP,
                updated_at TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS executions (
                id TEXT PRIMARY KEY,
                workflow_id TEXT NOT NULL,
                status TEXT NOT NULL,
                input_data TEXT,
                output_data TEXT,
                execution_log TEXT,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                error_message TEXT,
                FOREIGN KEY (workflow_id) REFERENCES workflows (id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def create_workflow(self, definition: WorkflowDefinition) -> str:
        """åˆ›å»ºå·¥ä½œæµ"""
        self.workflows[definition.id] = definition
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO workflows 
            (id, name, definition, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            definition.id,
            definition.name,
            json.dumps(asdict(definition)),
            definition.created_at,
            definition.updated_at
        ))
        
        conn.commit()
        conn.close()
        
        logger.info(f"Workflow created: {definition.name} ({definition.id})")
        return definition.id
    
    async def execute_workflow(self, workflow_id: str, 
                              input_data: Dict[str, Any] = None) -> str:
        """æ‰§è¡Œå·¥ä½œæµ"""
        if workflow_id not in self.workflows:
            raise ValueError(f"Workflow not found: {workflow_id}")
        
        workflow = self.workflows[workflow_id]
        execution = WorkflowExecution(
            id=str(uuid.uuid4()),
            workflow_id=workflow_id,
            status=WorkflowStatus.PENDING,
            start_time=datetime.now(),
            input_data=input_data or {}
        )
        
        self.executions[execution.id] = execution
        
        # å¼‚æ­¥æ‰§è¡Œå·¥ä½œæµ
        asyncio.create_task(self._run_workflow_execution(execution, workflow))
        
        return execution.id
    
    async def _run_workflow_execution(self, execution: WorkflowExecution, 
                                    workflow: WorkflowDefinition):
        """è¿è¡Œå·¥ä½œæµæ‰§è¡Œ"""
        try:
            execution.status = WorkflowStatus.RUNNING
            self._log_execution(execution, "Workflow execution started")
            
            # æ„å»ºLangGraph
            graph = self._build_langgraph(workflow)
            
            # åˆå§‹çŠ¶æ€
            initial_state: WorkflowState = {
                "execution_id": execution.id,
                "current_node": self._find_start_node(workflow),
                "data": execution.input_data.copy(),
                "context": {"workflow_id": workflow.id, "execution_id": execution.id},
                "errors": [],
                "iteration_count": 0,
                "start_time": time.time()
            }
            
            # æ‰§è¡Œå·¥ä½œæµ
            result = await graph.ainvoke(initial_state)
            
            # æ›´æ–°æ‰§è¡Œç»“æœ
            execution.status = WorkflowStatus.COMPLETED
            execution.end_time = datetime.now()
            execution.output_data = result.get("data", {})
            
            self._log_execution(execution, "Workflow execution completed successfully")
            
        except Exception as e:
            execution.status = WorkflowStatus.FAILED
            execution.end_time = datetime.now()
            execution.error_message = str(e)
            
            logger.error(f"Workflow execution failed: {e}")
            self._log_execution(execution, f"Workflow execution failed: {e}")
        
        # ä¿å­˜æ‰§è¡Œè®°å½•åˆ°æ•°æ®åº“
        self._save_execution_to_db(execution)
    
    def _build_langgraph(self, workflow: WorkflowDefinition) -> StateGraph:
        """æ„å»ºLangGraph"""
        graph = StateGraph(WorkflowState)
        
        # æ·»åŠ èŠ‚ç‚¹
        for node in workflow.nodes:
            if node.node_type == NodeType.INPUT:
                graph.add_node(node.id, self._create_input_node(node))
            elif node.node_type == NodeType.PROCESSING:
                graph.add_node(node.id, self._create_processing_node(node))
            elif node.node_type == NodeType.DECISION:
                graph.add_node(node.id, self._create_decision_node(node))
            elif node.node_type == NodeType.OUTPUT:
                graph.add_node(node.id, self._create_output_node(node))
            elif node.node_type == NodeType.CONDITION:
                graph.add_node(node.id, self._create_condition_node(node))
        
        # æ·»åŠ è¿æ¥
        for edge in workflow.edges:
            if edge.condition:
                graph.add_conditional_edges(
                    edge.from_node,
                    self._create_condition_function(edge.condition),
                    {True: edge.to_node, False: END}
                )
            else:
                graph.add_edge(edge.from_node, edge.to_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        start_node = self._find_start_node(workflow)
        if start_node:
            graph.set_entry_point(start_node)
        
        return graph.compile()
    
    def _create_input_node(self, node: WorkflowNode) -> Callable:
        """åˆ›å»ºè¾“å…¥èŠ‚ç‚¹"""
        async def input_node(state: WorkflowState) -> WorkflowState:
            try:
                # å¤„ç†è¾“å…¥æ•°æ®
                input_config = node.config
                input_type = input_config.get("type", "text")
                
                if input_type == "file" and "path" in state["data"]:
                    result = await self.processor.process_file(
                        state["data"]["path"], 
                        input_config.get("task", "analyze")
                    )
                    state["data"]["processed_input"] = result
                elif input_type == "json":
                    result = await self.processor.process_json(
                        state["data"], 
                        input_config.get("task", "validate")
                    )
                    state["data"]["processed_input"] = result
                else:
                    # æ–‡æœ¬è¾“å…¥
                    content = state["data"].get("content", "")
                    result = await self.processor.process_text(
                        content, 
                        input_config.get("task", "analyze")
                    )
                    state["data"]["processed_input"] = result
                
                state["current_node"] = node.id
                return state
                
            except Exception as e:
                state["errors"].append(f"Input node {node.id} failed: {e}")
                return state
        
        return input_node
    
    def _create_processing_node(self, node: WorkflowNode) -> Callable:
        """åˆ›å»ºå¤„ç†èŠ‚ç‚¹"""
        async def processing_node(state: WorkflowState) -> WorkflowState:
            try:
                processing_type = node.config.get("type", "transform")
                input_data = state["data"]
                
                if processing_type == "transform":
                    # æ•°æ®è½¬æ¢
                    result = self._transform_data(input_data, node.config)
                elif processing_type == "validate":
                    # æ•°æ®éªŒè¯
                    result = self._validate_data(input_data, node.config)
                elif processing_type == "enrich":
                    # æ•°æ®ä¸°å¯Œ
                    result = await self._enrich_data(input_data, node.config)
                else:
                    result = input_data
                
                state["data"][f"{node.id}_output"] = result
                state["current_node"] = node.id
                return state
                
            except Exception as e:
                state["errors"].append(f"Processing node {node.id} failed: {e}")
                return state
        
        return processing_node
    
    def _create_decision_node(self, node: WorkflowNode) -> Callable:
        """åˆ›å»ºå†³ç­–èŠ‚ç‚¹"""
        async def decision_node(state: WorkflowState) -> WorkflowState:
            try:
                question = node.config.get("question", "What should be the next action?")
                context = {
                    "current_data": state["data"],
                    "execution_context": state["context"],
                    "node_config": node.config
                }
                
                decision = await self.decision_engine.make_decision(context, question)
                
                state["data"][f"{node.id}_decision"] = decision
                state["current_node"] = node.id
                
                # æ ¹æ®å†³ç­–ç»“æœè®¾ç½®ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                if decision.get("decision", {}).get("next_node"):
                    state["context"]["next_node"] = decision["decision"]["next_node"]
                
                return state
                
            except Exception as e:
                state["errors"].append(f"Decision node {node.id} failed: {e}")
                return state
        
        return decision_node
    
    def _create_output_node(self, node: WorkflowNode) -> Callable:
        """åˆ›å»ºè¾“å‡ºèŠ‚ç‚¹"""
        async def output_node(state: WorkflowState) -> WorkflowState:
            try:
                output_config = node.config
                output_format = output_config.get("format", "json")
                
                if output_format == "json":
                    state["data"]["final_output"] = state["data"]
                elif output_format == "summary":
                    # ç”Ÿæˆæ‘˜è¦
                    summary = self._generate_summary(state["data"])
                    state["data"]["final_output"] = {"summary": summary}
                
                state["current_node"] = node.id
                return state
                
            except Exception as e:
                state["errors"].append(f"Output node {node.id} failed: {e}")
                return state
        
        return output_node
    
    def _create_condition_node(self, node: WorkflowNode) -> Callable:
        """åˆ›å»ºæ¡ä»¶èŠ‚ç‚¹"""
        async def condition_node(state: WorkflowState) -> WorkflowState:
            try:
                conditions = node.config.get("conditions", {})
                
                for condition_name, condition_expr in conditions.items():
                    result = self.decision_engine._safe_eval(condition_expr, state["data"])
                    state["data"][f"{node.id}_{condition_name}"] = result
                
                state["current_node"] = node.id
                return state
                
            except Exception as e:
                state["errors"].append(f"Condition node {node.id} failed: {e}")
                return state
        
        return condition_node
    
    def _create_condition_function(self, condition: str) -> Callable:
        """åˆ›å»ºæ¡ä»¶åˆ¤æ–­å‡½æ•°"""
        def condition_func(state: WorkflowState) -> bool:
            try:
                return self.decision_engine._safe_eval(condition, state["data"])
            except:
                return False
        
        return condition_func
    
    def _find_start_node(self, workflow: WorkflowDefinition) -> Optional[str]:
        """æŸ¥æ‰¾å¼€å§‹èŠ‚ç‚¹"""
        # æŸ¥æ‰¾æ²¡æœ‰è¾“å…¥çš„èŠ‚ç‚¹ä½œä¸ºå¼€å§‹èŠ‚ç‚¹
        all_to_nodes = {edge.to_node for edge in workflow.edges}
        for node in workflow.nodes:
            if node.id not in all_to_nodes:
                return node.id
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹
        for node in workflow.nodes:
            if node.node_type == NodeType.INPUT:
                return node.id
        
        # æœ€åè¿”å›ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
        return workflow.nodes[0].id if workflow.nodes else None
    
    def _transform_data(self, data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """æ•°æ®è½¬æ¢"""
        # è¿™é‡Œå®ç°æ•°æ®è½¬æ¢é€»è¾‘
        transformed = data.copy()
        
        # ç¤ºä¾‹è½¬æ¢è§„åˆ™
        rules = config.get("rules", [])
        for rule in rules:
            if rule["type"] == "rename":
                old_key = rule["from"]
                new_key = rule["to"]
                if old_key in transformed:
                    transformed[new_key] = transformed.pop(old_key)
            elif rule["type"] == "calculate":
                # ç®€å•è®¡ç®—
                expression = rule["expression"]
                try:
                    result = eval(expression, {"__builtins__": {}}, transformed)
                    transformed[rule["target"]] = result
                except:
                    pass
        
        return transformed
    
    def _validate_data(self, data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """æ•°æ®éªŒè¯"""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        required_fields = config.get("required_fields", [])
        for field in required_fields:
            if field not in data:
                validation_result["valid"] = False
                validation_result["errors"].append(f"Missing required field: {field}")
        
        type_checks = config.get("type_checks", {})
        for field, expected_type in type_checks.items():
            if field in data:
                actual_type = type(data[field]).__name__
                if actual_type != expected_type:
                    validation_result["warnings"].append(
                        f"Field {field} expected {expected_type}, got {actual_type}"
                    )
        
        return validation_result
    
    async def _enrich_data(self, data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """æ•°æ®ä¸°å¯Œ"""
        enriched = data.copy()
        
        # æ·»åŠ æ—¶é—´æˆ³
        if config.get("add_timestamp", False):
            enriched["timestamp"] = datetime.now().isoformat()
        
        # æ·»åŠ ID
        if config.get("add_id", False):
            enriched["id"] = str(uuid.uuid4())
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if config.get("add_stats", False):
            stats = {
                "field_count": len(enriched),
                "text_fields": len([v for v in enriched.values() if isinstance(v, str)]),
                "numeric_fields": len([v for v in enriched.values() if isinstance(v, (int, float))])
            }
            enriched["stats"] = stats
        
        return enriched
    
    def _generate_summary(self, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        summary_parts = []
        
        summary_parts.append(f"Data contains {len(data)} fields")
        
        for key, value in data.items():
            if isinstance(value, dict):
                summary_parts.append(f"{key}: object with {len(value)} properties")
            elif isinstance(value, list):
                summary_parts.append(f"{key}: array with {len(value)} items")
            elif isinstance(value, str):
                summary_parts.append(f"{key}: text ({len(value)} characters)")
            else:
                summary_parts.append(f"{key}: {type(value).__name__} value")
        
        return "; ".join(summary_parts)
    
    def _log_execution(self, execution: WorkflowExecution, message: str):
        """è®°å½•æ‰§è¡Œæ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "message": message,
            "status": execution.status.value
        }
        execution.execution_log.append(log_entry)
        logger.info(f"[{execution.id}] {message}")
    
    def _save_execution_to_db(self, execution: WorkflowExecution):
        """ä¿å­˜æ‰§è¡Œè®°å½•åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO executions 
            (id, workflow_id, status, input_data, output_data, execution_log, 
             start_time, end_time, error_message)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            execution.id,
            execution.workflow_id,
            execution.status.value,
            json.dumps(execution.input_data),
            json.dumps(execution.output_data),
            json.dumps(execution.execution_log),
            execution.start_time,
            execution.end_time,
            execution.error_message
        ))
        
        conn.commit()
        conn.close()
    
    async def get_execution_status(self, execution_id: str) -> Optional[WorkflowExecution]:
        """è·å–æ‰§è¡ŒçŠ¶æ€"""
        return self.executions.get(execution_id)
    
    async def cancel_execution(self, execution_id: str) -> bool:
        """å–æ¶ˆæ‰§è¡Œ"""
        execution = self.executions.get(execution_id)
        if execution and execution.status in [WorkflowStatus.PENDING, WorkflowStatus.RUNNING]:
            execution.status = WorkflowStatus.CANCELLED
            execution.end_time = datetime.now()
            self._log_execution(execution, "Execution cancelled by user")
            return True
        return False
    
    async def get_workflow_metrics(self, workflow_id: str) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµæŒ‡æ ‡"""
        executions = [e for e in self.executions.values() if e.workflow_id == workflow_id]
        
        if not executions:
            return {"error": "No executions found for this workflow"}
        
        total_executions = len(executions)
        completed = len([e for e in executions if e.status == WorkflowStatus.COMPLETED])
        failed = len([e for e in executions if e.status == WorkflowStatus.FAILED])
        
        avg_duration = 0
        completed_executions = [e for e in executions 
                              if e.status == WorkflowStatus.COMPLETED and e.end_time]
        if completed_executions:
            total_duration = sum(
                (e.end_time - e.start_time).total_seconds() 
                for e in completed_executions
            )
            avg_duration = total_duration / len(completed_executions)
        
        return {
            "workflow_id": workflow_id,
            "total_executions": total_executions,
            "completed": completed,
            "failed": failed,
            "success_rate": (completed / total_executions) * 100 if total_executions > 0 else 0,
            "avg_duration_seconds": avg_duration,
            "last_execution": max(executions, key=lambda x: x.start_time).start_time.isoformat()
        }


class SmartWorkflowDemo:
    """æ™ºèƒ½å·¥ä½œæµå¼•æ“æ¼”ç¤º"""
    
    def __init__(self):
        self.engine = WorkflowEngine()
        self._setup_decision_rules()
    
    def _setup_decision_rules(self):
        """è®¾ç½®å†³ç­–è§„åˆ™"""
        # æ·»åŠ ä¸€äº›ç¤ºä¾‹å†³ç­–è§„åˆ™
        self.engine.decision_engine.add_rule(
            name="high_priority_rule",
            condition="priority > 8",
            action="expedite",
            priority=10
        )
        
        self.engine.decision_engine.add_rule(
            name="error_handling_rule",
            condition="len(errors) > 0",
            action="retry_or_abort",
            priority=9
        )
        
        self.engine.decision_engine.add_rule(
            name="data_size_rule",
            condition="data_size > 1000000",
            action="batch_process",
            priority=5
        )
    
    async def run_demo(self):
        """è¿è¡Œæ¼”ç¤º"""
        print("ğŸ¤– æ™ºèƒ½å·¥ä½œæµå¼•æ“æ¼”ç¤º")
        print("=" * 60)
        
        try:
            await self._demonstrate_workflow_creation()
            await self._demonstrate_multimodal_processing()
            await self._demonstrate_decision_engine()
            await self._demonstrate_workflow_execution()
            await self._demonstrate_monitoring_and_metrics()
            
        except Exception as e:
            logger.error(f"Demo failed: {e}")
    
    async def _demonstrate_workflow_creation(self):
        """æ¼”ç¤ºå·¥ä½œæµåˆ›å»º"""
        print("\nğŸ“‹ å·¥ä½œæµåˆ›å»ºæ¼”ç¤º")
        print("-" * 40)
        
        # åˆ›å»ºæ•°æ®å¤„ç†å·¥ä½œæµ
        nodes = [
            WorkflowNode(
                id="input_1",
                name="æ•°æ®è¾“å…¥",
                node_type=NodeType.INPUT,
                config={"type": "json", "task": "validate"}
            ),
            WorkflowNode(
                id="process_1",
                name="æ•°æ®å¤„ç†",
                node_type=NodeType.PROCESSING,
                config={
                    "type": "transform",
                    "rules": [
                        {"type": "rename", "from": "old_name", "to": "new_name"}
                    ]
                }
            ),
            WorkflowNode(
                id="decision_1",
                name="æ™ºèƒ½å†³ç­–",
                node_type=NodeType.DECISION,
                config={"question": "Should we proceed with further processing?"}
            ),
            WorkflowNode(
                id="output_1",
                name="ç»“æœè¾“å‡º",
                node_type=NodeType.OUTPUT,
                config={"format": "summary"}
            )
        ]
        
        edges = [
            WorkflowEdge("input_1", "process_1"),
            WorkflowEdge("process_1", "decision_1"),
            WorkflowEdge("decision_1", "output_1")
        ]
        
        workflow = WorkflowDefinition(
            id="demo_workflow_1",
            name="æ•°æ®å¤„ç†å·¥ä½œæµ",
            description="æ¼”ç¤ºæ•°æ®è¾“å…¥ã€å¤„ç†ã€å†³ç­–å’Œè¾“å‡ºçš„å®Œæ•´æµç¨‹",
            version="1.0.0",
            nodes=nodes,
            edges=edges
        )
        
        workflow_id = await self.engine.create_workflow(workflow)
        print(f"âœ… åˆ›å»ºå·¥ä½œæµæˆåŠŸ: {workflow.name} (ID: {workflow_id})")
        print(f"   èŠ‚ç‚¹æ•°: {len(nodes)}")
        print(f"   è¿æ¥æ•°: {len(edges)}")
        
        return workflow_id
    
    async def _demonstrate_multimodal_processing(self):
        """æ¼”ç¤ºå¤šæ¨¡æ€å¤„ç†"""
        print("\nğŸ­ å¤šæ¨¡æ€æ•°æ®å¤„ç†æ¼”ç¤º")
        print("-" * 40)
        
        # æ–‡æœ¬å¤„ç†
        text_result = await self.engine.processor.process_text(
            "This is a sample text for analysis. It contains important information about our product.",
            "analyze"
        )
        print(f"ğŸ“ æ–‡æœ¬å¤„ç†ç»“æœ: {text_result['result'][:100]}...")
        
        # JSONå¤„ç†
        json_data = {
            "productName": "Smart Widget",
            "price": 99.99,
            "category": "Electronics",
            "features": ["WiFi", "Bluetooth", "Voice Control"],
            "availability": True
        }
        json_result = await self.engine.processor.process_json(json_data, "transform")
        print(f"ğŸ“Š JSONå¤„ç†ç»“æœ: {json_result['result']}")
        
        # æ–‡ä»¶å¤„ç†ï¼ˆå¦‚æœå­˜åœ¨README.mdï¼‰
        readme_path = "README.md"
        if os.path.exists(readme_path):
            file_result = await self.engine.processor.process_file(readme_path, "analyze")
            print(f"ğŸ“ æ–‡ä»¶å¤„ç†ç»“æœ: {file_result['result']['size']} bytes")
    
    async def _demonstrate_decision_engine(self):
        """æ¼”ç¤ºå†³ç­–å¼•æ“"""
        print("\nğŸ§  å†³ç­–å¼•æ“æ¼”ç¤º")
        print("-" * 40)
        
        # æµ‹è¯•ä¸åŒçš„å†³ç­–åœºæ™¯
        test_contexts = [
            {
                "name": "é«˜ä¼˜å…ˆçº§ä»»åŠ¡",
                "context": {"priority": 9, "data_size": 500, "errors": []},
                "question": "How should we handle this high priority task?"
            },
            {
                "name": "é”™è¯¯å¤„ç†åœºæ™¯",
                "context": {"priority": 5, "data_size": 1000, "errors": ["connection timeout"]},
                "question": "What should we do about the errors?"
            },
            {
                "name": "å¤§æ•°æ®å¤„ç†",
                "context": {"priority": 3, "data_size": 2000000, "errors": []},
                "question": "How to handle this large dataset?"
            }
        ]
        
        for test in test_contexts:
            decision = await self.engine.decision_engine.make_decision(
                test["context"], test["question"]
            )
            print(f"ğŸ¯ {test['name']}:")
            print(f"   å†³ç­–ç±»å‹: {decision['type']}")
            print(f"   ç½®ä¿¡åº¦: {decision['confidence']}")
            print(f"   æ¨ç†: {decision['reasoning']}")
            print()
    
    async def _demonstrate_workflow_execution(self):
        """æ¼”ç¤ºå·¥ä½œæµæ‰§è¡Œ"""
        print("\nğŸš€ å·¥ä½œæµæ‰§è¡Œæ¼”ç¤º")
        print("-" * 40)
        
        # åˆ›å»ºç®€å•çš„å·¥ä½œæµ
        workflow_id = await self._create_demo_workflow()
        
        # æ‰§è¡Œå·¥ä½œæµ
        input_data = {
            "content": "This is input data for processing",
            "old_name": "legacy_field",
            "priority": 7,
            "data_size": 1500
        }
        
        execution_id = await self.engine.execute_workflow(workflow_id, input_data)
        print(f"ğŸ¬ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {execution_id}")
        
        # ç­‰å¾…æ‰§è¡Œå®Œæˆ
        max_wait = 30  # æœ€å¤§ç­‰å¾…30ç§’
        wait_count = 0
        while wait_count < max_wait:
            execution = await self.engine.get_execution_status(execution_id)
            if execution and execution.status in [
                WorkflowStatus.COMPLETED, 
                WorkflowStatus.FAILED, 
                WorkflowStatus.CANCELLED
            ]:
                break
            await asyncio.sleep(1)
            wait_count += 1
        
        # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
        execution = await self.engine.get_execution_status(execution_id)
        if execution:
            print(f"ğŸ“Š æ‰§è¡ŒçŠ¶æ€: {execution.status.value}")
            if execution.status == WorkflowStatus.COMPLETED:
                print(f"âœ… æ‰§è¡ŒæˆåŠŸ!")
                print(f"   è¾“å‡ºæ•°æ®é”®æ•°: {len(execution.output_data)}")
                print(f"   æ‰§è¡Œæ—¥å¿—æ¡æ•°: {len(execution.execution_log)}")
            elif execution.status == WorkflowStatus.FAILED:
                print(f"âŒ æ‰§è¡Œå¤±è´¥: {execution.error_message}")
        
        return execution_id
    
    async def _create_demo_workflow(self) -> str:
        """åˆ›å»ºæ¼”ç¤ºå·¥ä½œæµ"""
        nodes = [
            WorkflowNode(
                id="input_demo",
                name="è¾“å…¥èŠ‚ç‚¹",
                node_type=NodeType.INPUT,
                config={"type": "text", "task": "analyze"}
            ),
            WorkflowNode(
                id="process_demo",
                name="å¤„ç†èŠ‚ç‚¹",
                node_type=NodeType.PROCESSING,
                config={
                    "type": "enrich",
                    "add_timestamp": True,
                    "add_stats": True
                }
            ),
            WorkflowNode(
                id="output_demo",
                name="è¾“å‡ºèŠ‚ç‚¹",
                node_type=NodeType.OUTPUT,
                config={"format": "summary"}
            )
        ]
        
        edges = [
            WorkflowEdge("input_demo", "process_demo"),
            WorkflowEdge("process_demo", "output_demo")
        ]
        
        workflow = WorkflowDefinition(
            id="simple_demo_workflow",
            name="ç®€å•æ¼”ç¤ºå·¥ä½œæµ",
            description="ç”¨äºæ¼”ç¤ºçš„ç®€å•å·¥ä½œæµ",
            version="1.0.0",
            nodes=nodes,
            edges=edges
        )
        
        return await self.engine.create_workflow(workflow)
    
    async def _demonstrate_monitoring_and_metrics(self):
        """æ¼”ç¤ºç›‘æ§å’ŒæŒ‡æ ‡"""
        print("\nğŸ“ˆ ç›‘æ§å’ŒæŒ‡æ ‡æ¼”ç¤º")
        print("-" * 40)
        
        # è·å–å·¥ä½œæµæŒ‡æ ‡
        for workflow_id in self.engine.workflows.keys():
            metrics = await self.engine.get_workflow_metrics(workflow_id)
            
            if "error" not in metrics:
                workflow_name = self.engine.workflows[workflow_id].name
                print(f"ğŸ“Š å·¥ä½œæµ: {workflow_name}")
                print(f"   æ€»æ‰§è¡Œæ¬¡æ•°: {metrics['total_executions']}")
                print(f"   æˆåŠŸæ¬¡æ•°: {metrics['completed']}")
                print(f"   å¤±è´¥æ¬¡æ•°: {metrics['failed']}")
                print(f"   æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
                if metrics['avg_duration_seconds'] > 0:
                    print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics['avg_duration_seconds']:.2f}ç§’")
                print()
        
        # æ˜¾ç¤ºç³»ç»Ÿæ¦‚è§ˆ
        total_workflows = len(self.engine.workflows)
        total_executions = len(self.engine.executions)
        
        print(f"ğŸ¯ ç³»ç»Ÿæ¦‚è§ˆ:")
        print(f"   å·¥ä½œæµæ€»æ•°: {total_workflows}")
        print(f"   æ‰§è¡Œæ€»æ•°: {total_executions}")
        
        if self.engine.executions:
            running_count = len([e for e in self.engine.executions.values() 
                               if e.status == WorkflowStatus.RUNNING])
            completed_count = len([e for e in self.engine.executions.values() 
                                 if e.status == WorkflowStatus.COMPLETED])
            print(f"   è¿è¡Œä¸­: {running_count}")
            print(f"   å·²å®Œæˆ: {completed_count}")


async def demo_workflow_engine():
    """æ™ºèƒ½å·¥ä½œæµå¼•æ“æ¼”ç¤ºå‡½æ•°ï¼ˆä¾›start.pyè°ƒç”¨ï¼‰"""
    demo = SmartWorkflowDemo()
    await demo.run_demo()


async def main():
    """ä¸»å‡½æ•°"""
    await demo_workflow_engine()


if __name__ == "__main__":
    asyncio.run(main())
